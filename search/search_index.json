{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 Welcome to the Tetrate Envoy and Service Mesh Immersion Day! Join the #kubecon-workshops channel in the Tetrate Community Slack channel for any questions. More resources \u00b6 Istio fundamentals course Envoy fundamentals course Certified Istio Administrator by Tetrate Tetrate Tech Talks","title":"Welcome!"},{"location":"#welcome","text":"Welcome to the Tetrate Envoy and Service Mesh Immersion Day! Join the #kubecon-workshops channel in the Tetrate Community Slack channel for any questions.","title":"Welcome!"},{"location":"#more-resources","text":"Istio fundamentals course Envoy fundamentals course Certified Istio Administrator by Tetrate Tetrate Tech Talks","title":"More resources"},{"location":"environment/","text":"Lab environment \u00b6 Options \u00b6 If you brought your own Kubernetes cluster: Kubernetes versions 1.16 through 1.23 should all work. For details, consult the Istio support status of Istio releases page for version 1.13.3. Although we typically recommend a 3-worker node cluster of machine type \"n1-standard-2\" or similar, a smaller cluster will work just fine for the Istio lab. If you have your own public cloud account: On GCP, the following command should provision a GKE cluster of adequate size for the workshop: gcloud container clusters create my-istio-cluster \\ --cluster-version latest \\ --machine-type \"n1-standard-2\" \\ --num-nodes \"3\" \\ --network \"default\" Feel free to provision a K8S cluster on any infrastructure of your choosing. Be sure to configure your ~/.kube/config file to point to your cluster. If you received Google credentials from the workshop instructors: A Kubernetes cluster has already been provisioned for you. Your instructor will demonstrate the process of accessing and configuring your environment, described below. The instructions below explain in detail how to access your account, select your project, and launch the cloud shell. If you are bringing your own Kubernetes cluster, you are ready to proceed with the Istio lab. Log in to GCP \u00b6 Log in to GCP using credentials provided by your instructor. Agree to the terms You will be prompted to select your country, click \"Agree and continue\" Select your project \u00b6 Select the GCP project you have been assigned, as follows: Click the project selector \"pulldown\" menu from the top banner, which will open a popup dialog Make sure the Select from organization is set to tetratelabs.com Select the tab named All You will see your GCP project name (istio-0to60..) listed under the organization tetratelabs.com Select the project from the list Verify that your project is selected: If you look in the banner now, you will see your selected project displayed. Launch the Cloud Shell \u00b6 The Google Cloud Shell will serve as your terminal environment for these labs. Click the Activate cloud shell icon (top right); the icon looks like this: A dialog may pop up, click Continue Your cloud shell terminal should appear at the bottom of the screen Feel free to expand the size of the cloud shell, or even open it in a separate window (locate the icon button in the terminal header, on the right) Warning Your connection to the Cloud Shell gets severed after a period of inactivity. Click on the Reconnect button when this happens. Configure cluster access \u00b6 Check that the kubectl CLI is installed kubectl version --short Generate a kubeconfig entry With the user interface From the command line Activate the top navigation menu ( Menu icon on the top left hand side of the page) Locate and click on the product Kubernetes Engine (you may have to scroll down until you see it) Your pre-provisioned 3-node Kubernetes cluster should appear in the main view Click on that row's \"three dot\" menu and select the Connect option A dialog prompt will appear with instructions Copy the gcloud command shown and paste it in your cloud shell gcloud container clusters get-credentials \\ $( gcloud container clusters list --format = \"value(name)\" ) \\ --zone $( gcloud container clusters list --format = \"value(location)\" ) \\ --project $( gcloud config get-value project ) Click Authorize when prompted The console message will state that a kubeconfig entry [was] generated for [your project] Verify that your Kubernetes context is set for your cluster kubectl config get-contexts Run a token command such as kubectl get node or kubectl get ns to ensure that you can communicate with the Kubernetes API Server. kubectl get ns All instructions in subsequent labs assume you will be working from the Google Cloud Shell. Tip This workshop makes extensive use of the kubectl CLI. Consider configuring an alias to make typing a little easier. cat << EOF >> ~/.bashrc source <(kubectl completion bash) alias k=kubectl complete -F __start_kubectl k EOF source ~/.bashrc","title":"Lab environment"},{"location":"environment/#lab-environment","text":"","title":"Lab environment"},{"location":"environment/#options","text":"If you brought your own Kubernetes cluster: Kubernetes versions 1.16 through 1.23 should all work. For details, consult the Istio support status of Istio releases page for version 1.13.3. Although we typically recommend a 3-worker node cluster of machine type \"n1-standard-2\" or similar, a smaller cluster will work just fine for the Istio lab. If you have your own public cloud account: On GCP, the following command should provision a GKE cluster of adequate size for the workshop: gcloud container clusters create my-istio-cluster \\ --cluster-version latest \\ --machine-type \"n1-standard-2\" \\ --num-nodes \"3\" \\ --network \"default\" Feel free to provision a K8S cluster on any infrastructure of your choosing. Be sure to configure your ~/.kube/config file to point to your cluster. If you received Google credentials from the workshop instructors: A Kubernetes cluster has already been provisioned for you. Your instructor will demonstrate the process of accessing and configuring your environment, described below. The instructions below explain in detail how to access your account, select your project, and launch the cloud shell. If you are bringing your own Kubernetes cluster, you are ready to proceed with the Istio lab.","title":"Options"},{"location":"environment/#log-in-to-gcp","text":"Log in to GCP using credentials provided by your instructor. Agree to the terms You will be prompted to select your country, click \"Agree and continue\"","title":"Log in to GCP"},{"location":"environment/#select-your-project","text":"Select the GCP project you have been assigned, as follows: Click the project selector \"pulldown\" menu from the top banner, which will open a popup dialog Make sure the Select from organization is set to tetratelabs.com Select the tab named All You will see your GCP project name (istio-0to60..) listed under the organization tetratelabs.com Select the project from the list Verify that your project is selected: If you look in the banner now, you will see your selected project displayed.","title":"Select your project"},{"location":"environment/#launch-the-cloud-shell","text":"The Google Cloud Shell will serve as your terminal environment for these labs. Click the Activate cloud shell icon (top right); the icon looks like this: A dialog may pop up, click Continue Your cloud shell terminal should appear at the bottom of the screen Feel free to expand the size of the cloud shell, or even open it in a separate window (locate the icon button in the terminal header, on the right) Warning Your connection to the Cloud Shell gets severed after a period of inactivity. Click on the Reconnect button when this happens.","title":"Launch the Cloud Shell"},{"location":"environment/#configure-cluster-access","text":"Check that the kubectl CLI is installed kubectl version --short Generate a kubeconfig entry With the user interface From the command line Activate the top navigation menu ( Menu icon on the top left hand side of the page) Locate and click on the product Kubernetes Engine (you may have to scroll down until you see it) Your pre-provisioned 3-node Kubernetes cluster should appear in the main view Click on that row's \"three dot\" menu and select the Connect option A dialog prompt will appear with instructions Copy the gcloud command shown and paste it in your cloud shell gcloud container clusters get-credentials \\ $( gcloud container clusters list --format = \"value(name)\" ) \\ --zone $( gcloud container clusters list --format = \"value(location)\" ) \\ --project $( gcloud config get-value project ) Click Authorize when prompted The console message will state that a kubeconfig entry [was] generated for [your project] Verify that your Kubernetes context is set for your cluster kubectl config get-contexts Run a token command such as kubectl get node or kubectl get ns to ensure that you can communicate with the Kubernetes API Server. kubectl get ns All instructions in subsequent labs assume you will be working from the Google Cloud Shell. Tip This workshop makes extensive use of the kubectl CLI. Consider configuring an alias to make typing a little easier. cat << EOF >> ~/.bashrc source <(kubectl completion bash) alias k=kubectl complete -F __start_kubectl k EOF source ~/.bashrc","title":"Configure cluster access"},{"location":"envoy/","text":"Introduction to Envoy \u00b6 In this lab, we will learn how basic Envoy building blocks are composed into a fully functioning Envoy proxy configuration. We begin with a minimal configuration needed to get Envoy up and to run and then build on it to get it to do more. Prerequisites \u00b6 We'll automatically download and run the Envoy proxy automatically, using the func-e CLI . To install func-e, run the following command: curl https://func-e.io/install.sh | bash -s -- -b /usr/local/bin You can verify func-e CLI gets installed by running the following command: func-e --version To run the Envoy proxy, we have to provide a configuration file. In the next section, we'll construct a minimal configuration file that allows us to run Envoy. Listening for requests \u00b6 The minimal configuration needed to launch Envoy includes a listener and filter chains . As the name suggests, the listener address and the port number are where Envoy will listen for incoming connections. In our example, we'll define a listener on address 0.0.0.0 and port 10000 and an array of empty filter chains (we'll get to them in a bit). minimal-config.yaml 1 2 3 4 5 6 7 8 static_resources : listeners : - name : listener_0 address : socket_address : address : 0.0.0.0 port_value : 10000 filter_chains : [{}] Save the above file to minimal-config.yaml , and let's run Envoy with this configuration: func-e run --config-path minimal-config.yaml & [2022-04-14 21:10:56.396][1328][warning][main] [source/server/server.cc:585] No admin address given, so no admin HTTP server started. [2022-04-14 21:10:56.396][1328][info][config] [source/server/configuration_impl.cc:127] loading tracing configuration [2022-04-14 21:10:56.396][1328][info][config] [source/server/configuration_impl.cc:87] loading 0 static secret(s) [2022-04-14 21:10:56.396][1328][info][config] [source/server/configuration_impl.cc:93] loading 0 cluster(s) [2022-04-14 21:10:56.396][1328][info][config] [source/server/configuration_impl.cc:97] loading 1 listener(s) [2022-04-14 21:10:56.397][1328][info][config] [source/server/configuration_impl.cc:109] loading stats configuration [2022-04-14 21:10:56.397][1328][info][runtime] [source/common/runtime/runtime_impl.cc:449] RTDS has finished initialization [2022-04-14 21:10:56.397][1328][info][upstream] [source/common/upstream/cluster_manager_impl.cc:206] cm init: all clusters initialized [2022-04-14 21:10:56.397][1328][warning][main] [source/server/server.cc:715] there is no configured limitto the number of allowed active connections. Set a limit via the runtime key overload.global_downstream_max_connections [2022-04-14 21:10:56.397][1328][info][main] [source/server/server.cc:817] all clusters initialized. initializing init manager [2022-04-14 21:10:56.397][1328][info][config] [source/server/listener_manager_impl.cc:779] all dependencies initialized. starting workers [2022-04-14 21:10:56.399][1328][info][main] [source/server/server.cc:836] starting main dispatch loop Running Envoy in background. Adding the character & at the end of the command will run Envoy in the background. To bring the Envoy process to the foreground, type fg in the terminal. To stop the process, use Ctrl + C When Envoy starts, it will print log messages to the console. We can tell how many secrets, clusters, listeners, and other resources Envoy loaded from the log messages. Since we only have a single listener, we'll see that Envoy loaded 0 secrets, 0 clusters, 1 listener. Envoy Admin Interface. You might have noticed the log message, No admin address given, so no admin HTTP server started. Envoy can be configured to run with an admin interface enabled. This web interface allows you to view Envoy's configuration and runtime information. We'll enable it later on. We can try and send a request to localhost:10000 , however since we're only listening to an address, but we haven't told Envoy where to route the requests to, there's not much we can expect from the response: $ curl -v localhost:10000 * Trying 127.0.0.1:10000... * Connected to localhost (127.0.0.1) port 10000 (#0) > GET / HTTP/1.1 > Host: localhost:10000 > User-Agent: curl/7.74.0 > Accept: */* > * Empty reply from server * Connection #0 to host localhost left intact curl: (52) Empty reply from server We get back Empty reply from server . Sending direct responses \u00b6 We'll have to define a filter chain to route the requests. Filter chains are a collection of filters that can be applied to a request. A filter can inspect the request at different levels and perform some action based on the results. There a various types of filters, such as listener filters, network filters, and HTTP filters, and they operate at different levels of the request. For example, the listener filters will operate on the received packet's headers. In contrast, the HTTP connection manager network filter can translate from raw bytes to HTTP-level messages. It can handle access logging, generate request IDs, manipulate headers, etc. We'll define the HTTP connection manager filter (or HCM for short) in the filter chain. Inside the HCM filter configuration, we can define one or more HTTP filters. The last filter in the HTTP filter chain has to be the router filter ( envoy.filters.http.router ) that implements the HTTP forwarding to a cluster. So we'll have a single network filter (HCM) with a single HTTP filter (router), as shown below: Review the following Envoy configuration, which adds an HCM filter specification to the chain (the symbols reveal explanations of the corresponding section of the configuration): static_resources : listeners : # (1) - name : listener_0 address : socket_address : address : 0.0.0.0 port_value : 10000 filter_chains : - filters : - name : envoy.filters.network.http_connection_manager # (2) typed_config : \"@type\" : type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix : hello_world_service # (3) http_filters : - name : envoy.filters.http.router # (4) route_config : # (5) name : my_first_route virtual_hosts : - name : my_vhost domains : [ \"*\" ] routes : - match : prefix : \"/\" direct_response : status : 200 body : inline_string : \"Hello!\" The listener section is the same as before; nothing has changed here. A network filter has a name, typed_config section that contains the configuration for the filter. The @type field is required and specifies the type of the filter. The stat_prefix is a required field that specifies the prefix for the stats generated for the filter. The only HTTP filter we've specified is the router filter. Route config contains the routing table for the connection manager. Other configuration settings The HCM contains numerous other fields we can specify and configure. You can check out the complete documentation of the HCM here . The most interesting part of the HCM is the route_config section. This section contains the route table for the connection manager, and it specifies the virtual hosts and routes that Envoy will use to route the requests. The domains field inside a virtual host specifies the domain names the virtual host will serve. This is where we could define an actual domain name we want to match against. For example, if we include hello.com in the domains array, Envoy will check if the host/authority header of the incoming request matches one of the specified domains and then proceed to the routes section. We're specifying * in the domains, which means we'll match on any host/authority header. Next, we can define one or more routes (note that the first route that matches will be used) by comparing the request properties such as the path, query parameters, headers, etc. Once the route is matched, we can specify the cluster Envoy forwards the request to. Later we'll use an actual cluster, but for this example, we're using a route called DirectResponseAction ( direct_response field) that returns a status and body we specify. Save the above configuration to direct-response.yaml and start Envoy: func-e run -c direct-response.yaml & After Envoy starts, we can send a request to localhost:10000 and we'll get back the response as specified in the direct_response field: $ curl -v localhost:10000 * Trying 127.0.0.1:10000... * Connected to localhost (127.0.0.1) port 10000 (#0) > GET / HTTP/1.1 > Host: localhost:10000 > User-Agent: curl/7.74.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 6 < content-type: text/plain < date: Thu, 14 Apr 2022 22:06:58 GMT < server: envoy < * Connection #0 to host localhost left intact Hello! Run fg to bring the Envoy process to the foreground, followed by Ctrl + C to interrupt and terminate the process. Defining endpoints \u00b6 Using a direct response when sending a request is an easy way to get started. However, it's not realistic. Typically, we want to route the request to endpoints. In Envoy, similar endpoints (e.g., multiple instances of the same service) are part of a logical group called clusters . Grouping the endpoints in such a way allows us to define the load balancing policies for the group and the locality of endpoints. Here's an example of a cluster called hello_world_cluster with two endpoints - both running on 127.0.0.1 , one listening on port 8100 and the other one listening on port 8200 : clusters : - name : hello_world_cluster load_assignment : cluster_name : hello_world_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8100 - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8200 The clusters in the Envoy configuration are declared in the clusters section, just like we declared the listeners in a separate field. The cluster section is also where we can configure health checks, circuit breakers, outlier detection, and other configuration settings. With the endpoints defined in clusters, we can combine the previous configuration and come up with something like this: clusters.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 static_resources : listeners : - name : listener_0 address : socket_address : address : 0.0.0.0 port_value : 10000 filter_chains : - filters : - name : envoy.filters.network.http_connection_manager typed_config : \"@type\" : type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix : hello_world_service http_filters : - name : envoy.filters.http.router route_config : name : my_first_route virtual_hosts : - name : my_vhost domains : [ \"*\" ] routes : - match : prefix : \"/\" route : cluster : hello_world_cluster clusters : - name : hello_world_cluster connect_timeout : 5s load_assignment : cluster_name : hello_world_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8100 - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8200 Save the above YAML to a file name clusters.yaml and run Envoy: func-e run -c clusters.yaml & If we send a request to localhost:10000 we'll get back an error: curl localhost:10000 [2022-04-15 18:19:18.006][1067][warning][client] [source/common/http/codec_client.cc:122] [C1] Connection is closed by peer during connecting. upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111 We get an error because nothing is listening on ports 8100 and 8200 , the two endpoints we referenced in the cluster. Let's just run two instances of the httpbin service: docker run -d -p 8100 :80 kennethreitz/httpbin docker run -d -p 8200 :80 kennethreitz/httpbin docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4b2beee7fec2 kennethreitz/httpbin \"gunicorn -b 0.0.0.0\u2026\" 3 seconds ago Up 2 seconds 0.0.0.0:8100->80/tcp condescending_meninsky 9af09f693a84 kennethreitz/httpbin \"gunicorn -b 0.0.0.0\u2026\" 5 seconds ago Up 2 seconds 0.0.0.0:8200->80/tcp reverent_kowalevski Now that we have two instances of the httpbin service running, we can send a request to localhost:10000 and we'll get back the response: curl localhost:10000/headers { \"headers\": { \"Accept\": \"*/*\", \"Host\": \"localhost:10000\", \"User-Agent\": \"curl/7.74.0\", \"X-Envoy-Expected-Rq-Timeout-Ms\": \"15000\" } } To see that Envoy is load-balancing between the two endpoints, we can stop one of the containers (e.g. docker stop <container_id> ) and then send a couple of requests. You'll notice we'll either get back a connection error or a response from one of the running services. Routing requests \u00b6 In the previous example, we've sent requests to multiple instances of the same service (a single cluster with two endpoints). In the following example, we'll see how to route requests to an instance of a different service. To represent different services, we can use another cluster. It's as simple as creating a separate cluster, for example: clusters : - name : hello_world_cluster load_assignment : cluster_name : hello_world_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8100 - name : second_cluster load_assignment : cluster_name : second_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8200 Once we have more than one cluster, we have to decide how we want to split the traffic between them. There are multiple ways we can configure traffic routing. Here are a couple of examples: Create multiple virtual hosts and split the traffic based on the host/authority (e.g., www.example.com traffic goes to one cluster and www.hello.com traffic goes to another cluster). Within a single virtual host, we can inspect the request properties and match (and route) traffic. For example, we can match the header values or match the request's path (e.g., /v1/api goes to one cluster and /v2/api goes to another cluster). Split the traffic by weight, where a certain percentage of traffic goes to one cluster and the rest to another cluster. Let's look at a simple example of splitting the traffic based on the path prefix. We'll use the same backend service (httpbin), but we'll route the traffic that starts with /one to the first cluster and the one that starts with /two to the second cluster. We'll rewrite the URL to /ip and /user-agent to see the difference between the two responses. Here's the snippet of the routing configuration: route_config : name : my_first_route virtual_hosts : - name : my_vhost domains : [ \"*\" ] routes : - match : # (1) path : \"/one\" route : prefix_rewrite : \"/ip\" # (2) cluster : hello_world_cluster - match : path : \"/two\" route : prefix_rewrite : \"/user-agent\" cluster : second_cluster We're matching the path to /one in the first route. If the path matches /one , we'll rewrite the path to /ip and send the request to the first cluster ( hello_world_cluster ). Why are we using prefix_rewrite ? We're rewriting the URL because the backend service ( httpbin ) doesn't implement the /one or /two paths. Therefore, we're matching that path at the proxy level, then based on the match, we're saying to rewrite the original path, for example, /one to /ip . We can put everything together now for the final example: routing.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 static_resources : listeners : - name : listener_0 address : socket_address : address : 0.0.0.0 port_value : 10000 filter_chains : - filters : - name : envoy.filters.network.http_connection_manager typed_config : \"@type\" : type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix : hello_world_service http_filters : - name : envoy.filters.http.router route_config : name : my_first_route virtual_hosts : - name : my_vhost domains : [ \"*\" ] routes : - match : path : \"/one\" route : prefix_rewrite : \"/ip\" cluster : hello_world_cluster - match : path : \"/two\" route : prefix_rewrite : \"/user-agent\" cluster : second_cluster clusters : - name : hello_world_cluster connect_timeout : 5s load_assignment : cluster_name : hello_world_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8100 - name : second_cluster connect_timeout : 5s load_assignment : cluster_name : second_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8200 Save the above YAML to routing.yaml and run Envoy: func-e run -c routing.yaml & Let's try sending the request to localhost:10000/one - we'll get back the following response: curl localhost:10000/one { \"origin\": \"172.18.0.1\" } Similarly, if we send a request to localhost:10000/two we'll get back the user agent information: curl localhost:10000/two { \"user-agent\": \"curl/7.74.0\" } Type fg to bring the Envoy process to the foreground and press Ctrl + C to stop it. Cleanup \u00b6 To stop running the Docker containers, run: docker stop $( docker ps -q ) Summary \u00b6 Equipped with the basics of Envoy, in the next lab, we turn our attention to Istio, of which Envoy is a crucial building block.","title":"Introduction to Envoy"},{"location":"envoy/#introduction-to-envoy","text":"In this lab, we will learn how basic Envoy building blocks are composed into a fully functioning Envoy proxy configuration. We begin with a minimal configuration needed to get Envoy up and to run and then build on it to get it to do more.","title":"Introduction to Envoy"},{"location":"envoy/#prerequisites","text":"We'll automatically download and run the Envoy proxy automatically, using the func-e CLI . To install func-e, run the following command: curl https://func-e.io/install.sh | bash -s -- -b /usr/local/bin You can verify func-e CLI gets installed by running the following command: func-e --version To run the Envoy proxy, we have to provide a configuration file. In the next section, we'll construct a minimal configuration file that allows us to run Envoy.","title":"Prerequisites"},{"location":"envoy/#listening-for-requests","text":"The minimal configuration needed to launch Envoy includes a listener and filter chains . As the name suggests, the listener address and the port number are where Envoy will listen for incoming connections. In our example, we'll define a listener on address 0.0.0.0 and port 10000 and an array of empty filter chains (we'll get to them in a bit). minimal-config.yaml 1 2 3 4 5 6 7 8 static_resources : listeners : - name : listener_0 address : socket_address : address : 0.0.0.0 port_value : 10000 filter_chains : [{}] Save the above file to minimal-config.yaml , and let's run Envoy with this configuration: func-e run --config-path minimal-config.yaml & [2022-04-14 21:10:56.396][1328][warning][main] [source/server/server.cc:585] No admin address given, so no admin HTTP server started. [2022-04-14 21:10:56.396][1328][info][config] [source/server/configuration_impl.cc:127] loading tracing configuration [2022-04-14 21:10:56.396][1328][info][config] [source/server/configuration_impl.cc:87] loading 0 static secret(s) [2022-04-14 21:10:56.396][1328][info][config] [source/server/configuration_impl.cc:93] loading 0 cluster(s) [2022-04-14 21:10:56.396][1328][info][config] [source/server/configuration_impl.cc:97] loading 1 listener(s) [2022-04-14 21:10:56.397][1328][info][config] [source/server/configuration_impl.cc:109] loading stats configuration [2022-04-14 21:10:56.397][1328][info][runtime] [source/common/runtime/runtime_impl.cc:449] RTDS has finished initialization [2022-04-14 21:10:56.397][1328][info][upstream] [source/common/upstream/cluster_manager_impl.cc:206] cm init: all clusters initialized [2022-04-14 21:10:56.397][1328][warning][main] [source/server/server.cc:715] there is no configured limitto the number of allowed active connections. Set a limit via the runtime key overload.global_downstream_max_connections [2022-04-14 21:10:56.397][1328][info][main] [source/server/server.cc:817] all clusters initialized. initializing init manager [2022-04-14 21:10:56.397][1328][info][config] [source/server/listener_manager_impl.cc:779] all dependencies initialized. starting workers [2022-04-14 21:10:56.399][1328][info][main] [source/server/server.cc:836] starting main dispatch loop Running Envoy in background. Adding the character & at the end of the command will run Envoy in the background. To bring the Envoy process to the foreground, type fg in the terminal. To stop the process, use Ctrl + C When Envoy starts, it will print log messages to the console. We can tell how many secrets, clusters, listeners, and other resources Envoy loaded from the log messages. Since we only have a single listener, we'll see that Envoy loaded 0 secrets, 0 clusters, 1 listener. Envoy Admin Interface. You might have noticed the log message, No admin address given, so no admin HTTP server started. Envoy can be configured to run with an admin interface enabled. This web interface allows you to view Envoy's configuration and runtime information. We'll enable it later on. We can try and send a request to localhost:10000 , however since we're only listening to an address, but we haven't told Envoy where to route the requests to, there's not much we can expect from the response: $ curl -v localhost:10000 * Trying 127.0.0.1:10000... * Connected to localhost (127.0.0.1) port 10000 (#0) > GET / HTTP/1.1 > Host: localhost:10000 > User-Agent: curl/7.74.0 > Accept: */* > * Empty reply from server * Connection #0 to host localhost left intact curl: (52) Empty reply from server We get back Empty reply from server .","title":"Listening for requests"},{"location":"envoy/#sending-direct-responses","text":"We'll have to define a filter chain to route the requests. Filter chains are a collection of filters that can be applied to a request. A filter can inspect the request at different levels and perform some action based on the results. There a various types of filters, such as listener filters, network filters, and HTTP filters, and they operate at different levels of the request. For example, the listener filters will operate on the received packet's headers. In contrast, the HTTP connection manager network filter can translate from raw bytes to HTTP-level messages. It can handle access logging, generate request IDs, manipulate headers, etc. We'll define the HTTP connection manager filter (or HCM for short) in the filter chain. Inside the HCM filter configuration, we can define one or more HTTP filters. The last filter in the HTTP filter chain has to be the router filter ( envoy.filters.http.router ) that implements the HTTP forwarding to a cluster. So we'll have a single network filter (HCM) with a single HTTP filter (router), as shown below: Review the following Envoy configuration, which adds an HCM filter specification to the chain (the symbols reveal explanations of the corresponding section of the configuration): static_resources : listeners : # (1) - name : listener_0 address : socket_address : address : 0.0.0.0 port_value : 10000 filter_chains : - filters : - name : envoy.filters.network.http_connection_manager # (2) typed_config : \"@type\" : type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix : hello_world_service # (3) http_filters : - name : envoy.filters.http.router # (4) route_config : # (5) name : my_first_route virtual_hosts : - name : my_vhost domains : [ \"*\" ] routes : - match : prefix : \"/\" direct_response : status : 200 body : inline_string : \"Hello!\" The listener section is the same as before; nothing has changed here. A network filter has a name, typed_config section that contains the configuration for the filter. The @type field is required and specifies the type of the filter. The stat_prefix is a required field that specifies the prefix for the stats generated for the filter. The only HTTP filter we've specified is the router filter. Route config contains the routing table for the connection manager. Other configuration settings The HCM contains numerous other fields we can specify and configure. You can check out the complete documentation of the HCM here . The most interesting part of the HCM is the route_config section. This section contains the route table for the connection manager, and it specifies the virtual hosts and routes that Envoy will use to route the requests. The domains field inside a virtual host specifies the domain names the virtual host will serve. This is where we could define an actual domain name we want to match against. For example, if we include hello.com in the domains array, Envoy will check if the host/authority header of the incoming request matches one of the specified domains and then proceed to the routes section. We're specifying * in the domains, which means we'll match on any host/authority header. Next, we can define one or more routes (note that the first route that matches will be used) by comparing the request properties such as the path, query parameters, headers, etc. Once the route is matched, we can specify the cluster Envoy forwards the request to. Later we'll use an actual cluster, but for this example, we're using a route called DirectResponseAction ( direct_response field) that returns a status and body we specify. Save the above configuration to direct-response.yaml and start Envoy: func-e run -c direct-response.yaml & After Envoy starts, we can send a request to localhost:10000 and we'll get back the response as specified in the direct_response field: $ curl -v localhost:10000 * Trying 127.0.0.1:10000... * Connected to localhost (127.0.0.1) port 10000 (#0) > GET / HTTP/1.1 > Host: localhost:10000 > User-Agent: curl/7.74.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 6 < content-type: text/plain < date: Thu, 14 Apr 2022 22:06:58 GMT < server: envoy < * Connection #0 to host localhost left intact Hello! Run fg to bring the Envoy process to the foreground, followed by Ctrl + C to interrupt and terminate the process.","title":"Sending direct responses"},{"location":"envoy/#defining-endpoints","text":"Using a direct response when sending a request is an easy way to get started. However, it's not realistic. Typically, we want to route the request to endpoints. In Envoy, similar endpoints (e.g., multiple instances of the same service) are part of a logical group called clusters . Grouping the endpoints in such a way allows us to define the load balancing policies for the group and the locality of endpoints. Here's an example of a cluster called hello_world_cluster with two endpoints - both running on 127.0.0.1 , one listening on port 8100 and the other one listening on port 8200 : clusters : - name : hello_world_cluster load_assignment : cluster_name : hello_world_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8100 - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8200 The clusters in the Envoy configuration are declared in the clusters section, just like we declared the listeners in a separate field. The cluster section is also where we can configure health checks, circuit breakers, outlier detection, and other configuration settings. With the endpoints defined in clusters, we can combine the previous configuration and come up with something like this: clusters.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 static_resources : listeners : - name : listener_0 address : socket_address : address : 0.0.0.0 port_value : 10000 filter_chains : - filters : - name : envoy.filters.network.http_connection_manager typed_config : \"@type\" : type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix : hello_world_service http_filters : - name : envoy.filters.http.router route_config : name : my_first_route virtual_hosts : - name : my_vhost domains : [ \"*\" ] routes : - match : prefix : \"/\" route : cluster : hello_world_cluster clusters : - name : hello_world_cluster connect_timeout : 5s load_assignment : cluster_name : hello_world_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8100 - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8200 Save the above YAML to a file name clusters.yaml and run Envoy: func-e run -c clusters.yaml & If we send a request to localhost:10000 we'll get back an error: curl localhost:10000 [2022-04-15 18:19:18.006][1067][warning][client] [source/common/http/codec_client.cc:122] [C1] Connection is closed by peer during connecting. upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111 We get an error because nothing is listening on ports 8100 and 8200 , the two endpoints we referenced in the cluster. Let's just run two instances of the httpbin service: docker run -d -p 8100 :80 kennethreitz/httpbin docker run -d -p 8200 :80 kennethreitz/httpbin docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4b2beee7fec2 kennethreitz/httpbin \"gunicorn -b 0.0.0.0\u2026\" 3 seconds ago Up 2 seconds 0.0.0.0:8100->80/tcp condescending_meninsky 9af09f693a84 kennethreitz/httpbin \"gunicorn -b 0.0.0.0\u2026\" 5 seconds ago Up 2 seconds 0.0.0.0:8200->80/tcp reverent_kowalevski Now that we have two instances of the httpbin service running, we can send a request to localhost:10000 and we'll get back the response: curl localhost:10000/headers { \"headers\": { \"Accept\": \"*/*\", \"Host\": \"localhost:10000\", \"User-Agent\": \"curl/7.74.0\", \"X-Envoy-Expected-Rq-Timeout-Ms\": \"15000\" } } To see that Envoy is load-balancing between the two endpoints, we can stop one of the containers (e.g. docker stop <container_id> ) and then send a couple of requests. You'll notice we'll either get back a connection error or a response from one of the running services.","title":"Defining endpoints"},{"location":"envoy/#routing-requests","text":"In the previous example, we've sent requests to multiple instances of the same service (a single cluster with two endpoints). In the following example, we'll see how to route requests to an instance of a different service. To represent different services, we can use another cluster. It's as simple as creating a separate cluster, for example: clusters : - name : hello_world_cluster load_assignment : cluster_name : hello_world_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8100 - name : second_cluster load_assignment : cluster_name : second_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8200 Once we have more than one cluster, we have to decide how we want to split the traffic between them. There are multiple ways we can configure traffic routing. Here are a couple of examples: Create multiple virtual hosts and split the traffic based on the host/authority (e.g., www.example.com traffic goes to one cluster and www.hello.com traffic goes to another cluster). Within a single virtual host, we can inspect the request properties and match (and route) traffic. For example, we can match the header values or match the request's path (e.g., /v1/api goes to one cluster and /v2/api goes to another cluster). Split the traffic by weight, where a certain percentage of traffic goes to one cluster and the rest to another cluster. Let's look at a simple example of splitting the traffic based on the path prefix. We'll use the same backend service (httpbin), but we'll route the traffic that starts with /one to the first cluster and the one that starts with /two to the second cluster. We'll rewrite the URL to /ip and /user-agent to see the difference between the two responses. Here's the snippet of the routing configuration: route_config : name : my_first_route virtual_hosts : - name : my_vhost domains : [ \"*\" ] routes : - match : # (1) path : \"/one\" route : prefix_rewrite : \"/ip\" # (2) cluster : hello_world_cluster - match : path : \"/two\" route : prefix_rewrite : \"/user-agent\" cluster : second_cluster We're matching the path to /one in the first route. If the path matches /one , we'll rewrite the path to /ip and send the request to the first cluster ( hello_world_cluster ). Why are we using prefix_rewrite ? We're rewriting the URL because the backend service ( httpbin ) doesn't implement the /one or /two paths. Therefore, we're matching that path at the proxy level, then based on the match, we're saying to rewrite the original path, for example, /one to /ip . We can put everything together now for the final example: routing.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 static_resources : listeners : - name : listener_0 address : socket_address : address : 0.0.0.0 port_value : 10000 filter_chains : - filters : - name : envoy.filters.network.http_connection_manager typed_config : \"@type\" : type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix : hello_world_service http_filters : - name : envoy.filters.http.router route_config : name : my_first_route virtual_hosts : - name : my_vhost domains : [ \"*\" ] routes : - match : path : \"/one\" route : prefix_rewrite : \"/ip\" cluster : hello_world_cluster - match : path : \"/two\" route : prefix_rewrite : \"/user-agent\" cluster : second_cluster clusters : - name : hello_world_cluster connect_timeout : 5s load_assignment : cluster_name : hello_world_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8100 - name : second_cluster connect_timeout : 5s load_assignment : cluster_name : second_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8200 Save the above YAML to routing.yaml and run Envoy: func-e run -c routing.yaml & Let's try sending the request to localhost:10000/one - we'll get back the following response: curl localhost:10000/one { \"origin\": \"172.18.0.1\" } Similarly, if we send a request to localhost:10000/two we'll get back the user agent information: curl localhost:10000/two { \"user-agent\": \"curl/7.74.0\" } Type fg to bring the Envoy process to the foreground and press Ctrl + C to stop it.","title":"Routing requests"},{"location":"envoy/#cleanup","text":"To stop running the Docker containers, run: docker stop $( docker ps -q )","title":"Cleanup"},{"location":"envoy/#summary","text":"Equipped with the basics of Envoy, in the next lab, we turn our attention to Istio, of which Envoy is a crucial building block.","title":"Summary"},{"location":"install/","text":"Install Istio \u00b6 In this lab you will install Istio. Download Istio \u00b6 Run the following command from your home directory. curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .13.3 sh - Navigate into the directory created by the above command. cd istio-1.13.3 Add istioctl to your PATH \u00b6 The istioctl CLI is located in the bin/ subdirectory. Note Cloud Shell only preserves files located inside your home directory across sessions. This means that if you install a binary to a PATH such as /usr/local/bin , after your session times out that file will no longer be there! As a workaround, you will add ${HOME}/bin to your PATH and place the binary there. Create a bin subdirectory in your home directory: mkdir ~/bin Copy the CLI to that subdirectory: cp ./bin/istioctl ~/bin Add your home bin subdirectory to your PATH cat << EOF >> ~/.bashrc export PATH=\"~/bin:\\$PATH\" EOF And then: source ~/.bashrc Verify that istioctl is installed with: istioctl version With the CLI installed, proceed to install Istio to Kubernetes. Install Istio \u00b6 Istio can be installed directly with the CLI: istioctl install When prompted, enter y to proceed to install Istio. Take a moment to learn more about Istio installation profiles . Verify that Istio is installed \u00b6 List Kubernetes namespaces and note the new namespace istio-system kubectl get ns Verify that the istiod controller pod is running in that namespace kubectl get pod -n istio-system Re-run istioctl version . The output should include a control plane version, indicating that Istio is indeed present in the cluster.","title":"Install Istio"},{"location":"install/#install-istio","text":"In this lab you will install Istio.","title":"Install Istio"},{"location":"install/#download-istio","text":"Run the following command from your home directory. curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .13.3 sh - Navigate into the directory created by the above command. cd istio-1.13.3","title":"Download Istio"},{"location":"install/#add-istioctl-to-your-path","text":"The istioctl CLI is located in the bin/ subdirectory. Note Cloud Shell only preserves files located inside your home directory across sessions. This means that if you install a binary to a PATH such as /usr/local/bin , after your session times out that file will no longer be there! As a workaround, you will add ${HOME}/bin to your PATH and place the binary there. Create a bin subdirectory in your home directory: mkdir ~/bin Copy the CLI to that subdirectory: cp ./bin/istioctl ~/bin Add your home bin subdirectory to your PATH cat << EOF >> ~/.bashrc export PATH=\"~/bin:\\$PATH\" EOF And then: source ~/.bashrc Verify that istioctl is installed with: istioctl version With the CLI installed, proceed to install Istio to Kubernetes.","title":"Add istioctl to your PATH"},{"location":"install/#install-istio_1","text":"Istio can be installed directly with the CLI: istioctl install When prompted, enter y to proceed to install Istio. Take a moment to learn more about Istio installation profiles .","title":"Install Istio"},{"location":"install/#verify-that-istio-is-installed","text":"List Kubernetes namespaces and note the new namespace istio-system kubectl get ns Verify that the istiod controller pod is running in that namespace kubectl get pod -n istio-system Re-run istioctl version . The output should include a control plane version, indicating that Istio is indeed present in the cluster.","title":"Verify that Istio is installed"},{"location":"istio/","text":"Introduction to Istio \u00b6 Preface \u00b6 In the Envoy lab, we explored two scenarios: A single Envoy \"cluster\" with two endpoints. In this scenario, we observed that a request to the proxy resulted in the load-balancing of requests across the two endpoints. Two Envoy clusters, together with a routing configuration to route requests from the proxy to either cluster depending on the request's path prefix: Requests having the path prefix of /one were routed to the first cluster's /ip endpoint, and Requests with the path prefix of /two were routed to the second cluster's /user-agent endpoint. In this lab, you will learn how to model both scenarios in the context of Istio. Envoy is a building block of Istio. In Istio, Envoy proxies are configured indirectly, using a combination of: Implicit information drawn from the Kubernetes environment, and Istio-specific Kubernetes custom resources. Environments \u00b6 See options for environments. Install Istio \u00b6 Follow these instructions to install Istio in your environment. Download lab artifacts \u00b6 Use the following command to download (to a subdirectory named istio-artifacts ) a copy of all yaml manifests necessary for this lab. git clone https://github.com/tetratelabs/kubecon2022-eu-immersion-day.git && \\ mv kubecon2022-eu-immersion-day/artifacts/istio ./istio-artifacts && \\ rm -rf kubecon2022-eu-immersion-day Where are the Envoys? \u00b6 In Istio, Envoy proxy instances are present in two distinct locations: In the heart of the mesh : they are bundled as sidecar containers in the pods that run our workloads. At the edge : as standalone gateways handling ingress and egress traffic in and out of the mesh. An ingress gateway is deployed as part of the installation of Istio. It resides in the istio-system namespace. Verify this: kubectl get deploy -n istio-system To deploy Envoy as a sidecar, we will employ the convenient automatic sidecar injection , which works as follows: Label the target namespace with the special label istio-injection with the value enabled : kubectl label ns default istio-injection = enabled Verify: kubectl get ns -Listio-injection When using kubectl to apply a deployment, Istio employs a Kubernetes admission controller to augment the pod specification to bundle Envoy into a sidecar container. Verify this: observe the presence of the istio sidecar injector in your Kubernetes cluster: kubectl get mutatingwebhookconfigurations Turn on Envoy access logging \u00b6 Turn on access logging in Envoy, by applying the following Telemetry custom resource: access-logging.yaml 1 2 3 4 5 6 7 8 9 apiVersion : telemetry.istio.io/v1alpha1 kind : Telemetry metadata : name : mesh-default namespace : istio-system spec : accessLogging : - providers : - name : envoy kubectl apply -f access-logging.yaml This will simplify our ability to observe http requests in the mesh. What is Telemetry resource? The Telemetry resource is a Kubernetes custom resource that defines how the telemetry is generated for workloads within the mesh. Scenario 1: Load-balancing across multiple endpoints \u00b6 Deploy httpbin \u00b6 As in the previous lab, we use httpbin as the application under test. Istio conveniently provides httpbin as one of its sample applications . For convenience, you will find a copy of the httpbin.yaml Kubernetes manifest in the istio-artifacts folder. Deploy httpbin to the default namespace: httpbin.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # httpbin service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : httpbin --- apiVersion : v1 kind : Service metadata : name : httpbin labels : app : httpbin service : httpbin spec : ports : - name : http port : 8000 targetPort : 80 selector : app : httpbin --- apiVersion : apps/v1 kind : Deployment metadata : name : httpbin spec : replicas : 1 selector : matchLabels : app : httpbin version : v1 template : metadata : labels : app : httpbin version : v1 spec : serviceAccountName : httpbin containers : - image : docker.io/kennethreitz/httpbin imagePullPolicy : IfNotPresent name : httpbin ports : - containerPort : 80 kubectl apply -f httpbin.yaml Scale httpbin \u00b6 kubectl scale deploy httpbin --replicas = 2 Having two pods will give us the two endpoints to load-balance across. Deploy the sleep client \u00b6 Istio also provides a convenient sample app named sleep . Deploy the sleep client: sleep.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # Sleep service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : sleep --- apiVersion : v1 kind : Service metadata : name : sleep labels : app : sleep service : sleep spec : ports : - port : 80 name : http selector : app : sleep --- apiVersion : apps/v1 kind : Deployment metadata : name : sleep spec : replicas : 1 selector : matchLabels : app : sleep template : metadata : labels : app : sleep spec : terminationGracePeriodSeconds : 0 serviceAccountName : sleep containers : - name : sleep image : curlimages/curl command : [ \"/bin/sleep\" , \"3650d\" ] imagePullPolicy : IfNotPresent volumeMounts : - mountPath : /etc/sleep/tls name : secret-volume volumes : - name : secret-volume secret : secretName : sleep-secret optional : true --- kubectl apply -f sleep.yaml Challenge \u00b6 Observe that all pods in the default namespace each have two containers: kubectl get pod -n default Can you discover the name of the sidecar container? Hint Describe any of the pods in the default namespace and study the Containers section. Observe load-balancing between the two endpoints \u00b6 Requests from sleep are load-balanced across the two httpbin endpoints. Note In the commands below, we capture the names of each of the two httpbin pods and of the sleep pod independently, for clarity. Tail the logs of each Envoy sidecar on the receiving end. In one terminal, run: HTTPBIN_POD_1 = $( kubectl get pod -l app = httpbin -ojsonpath = '{.items[0].metadata.name}' ) kubectl logs --follow $HTTPBIN_POD_1 -c istio-proxy Note Note above how the name of the container istio-proxy is used to reference the sidecar. In a second terminal, run: HTTPBIN_POD_2 = $( kubectl get pod -l app = httpbin -ojsonpath = '{.items[1].metadata.name}' ) kubectl logs --follow $HTTPBIN_POD_2 -c istio-proxy Make repeated calls from the sleep container to the httbin service and observe which of the two httpbin pods receives the request. SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) kubectl exec $SLEEP_POD -it -- curl httpbin:8000/html You can stop following the logs by pressing Ctrl + C and close the first two terminal windows. Behind the curtain \u00b6 The Istio CLI, istioctl , provides a handy subcommand proxy-config , that will help us get at the configuration of the Envoy proxy in the sleep pod: its listeners, routes, clusters, and endpoints. Capture the name of the sleep pod to a variable: SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) Envoy's listeners configuration \u00b6 Run the following command: istioctl proxy-config listener $SLEEP_POD The output displays a high-level overview of the Envoy listener configuration. From this output we learn that Envoy has multiple listeners, listening on multiple ports. Some listeners handle inbound requests, for example there's a health endpoint on port 15021 , and a prometheus scrape endpoint on port 15090 . The listener on port 8000 (which matches the port number of the httpbin cluster IP service) is responsible for handling requests bound to the httpbin service. To see the full listener section of the Envoy configuration for port 8000 , run: istioctl proxy-config listener $SLEEP_POD --port 8000 -o yaml > listeners.yaml The output is voluminous (~ 200+ lines) and that's why we piped it into the listeners.yaml file. Note the following: trafficDirection (at the very end of the output) is set to OUTBOUND The address section specifies the address and port that the listener is configured for: address : socketAddress : address : 0.0.0.0 portValue : 8000 The configuration contains a filterChains field: filterChains : - filterChainMatch : applicationProtocols : ... The filter chain contains a filter named envoy.filters.network.http_connection_manager , and its list of httpFilters ends with the router filter: httpFilters : - name : istio.metadata_exchange - ... - name : envoy.filters.http.router typedConfig : '@type' : type.googleapis.com/envoy.extensions.filters.http.router.v3.Router All of the above facts should match with what you learned in the Introduction to Envoy . Routes \u00b6 Similar to the proxy-config listener command, the high-level overview for routes is the following command: istioctl proxy-config route $SLEEP_POD Zero-in on the route configuration for port 8000 : istioctl proxy-config route $SLEEP_POD --name 8000 -o yaml The output will show the route configuration, including this section: ... routes : - decorator : operation : httpbin.default.svc.cluster.local:8000/* match : prefix : / name : default route : cluster : outbound|8000||httpbin.default.svc.cluster.local ... The above output states that calls to the httpbin service should be routed to the cluster named outbound|8000||httpbin.default.svc.cluster.local . Clusters \u00b6 We can view all Envoy clusters with: istioctl proxy-config cluster $SLEEP_POD And specifically look at the configuration for the httpbin cluster with: istioctl proxy-config cluster $SLEEP_POD --fqdn httpbin.default.svc.cluster.local -o yaml Endpoints \u00b6 More importantly, we'd like to know what are the endpoints backing the httpbin cluster. istioctl proxy-config endpoint $SLEEP_POD --cluster \"outbound|8000||httpbin.default.svc.cluster.local\" Verify that the endpoint addresses from the output in fact match the pod IPs of the httpbin workloads: kubectl get pod -l app = httpbin -o wide Destination Rules \u00b6 With Istio, you can apply the DestinationRule CRD (Custom Resource Definition) to configure traffic policy: the details of how clients call a service. Specifically, you can configure: Load balancer settings : which load balancing algorithm to use Connection pool settings : for both tcp and http connections, configure the volume of connections, retries, timeouts, etc.. Outlier detection : under what conditions to evict an unhealthy endpoints, and for how long TLS mode : whether a connection to an upstream service should use plain text, TLS, mutual TLS using certificates you specify, or mutual TLS using Istio-issued certificates. Explore applying a destination rule to alter the load balancer configuration. Did you know? What is the default load balancing algorithm currently in play for calls to httpbin ? Visit the Istio configuration reference here to find out. Apply the following destination rule for the httpbin service, which alters the load balancing algorithm to LEAST_CONN : destination-rule.yaml 1 2 3 4 5 6 7 8 9 10 --- apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : httbin spec : host : httpbin.default.svc.cluster.local trafficPolicy : loadBalancer : simple : LEAST_CONN In Envoy, the load balancer policy is associated to a given upstream service, in Envoy's terms, it's in the \"cluster\" config. Look for lbPolicy field in cluster configuration YAML output: istioctl proxy-config cluster $SLEEP_POD --fqdn httpbin.default.svc.cluster.local -o yaml | grep lbPolicy -A 3 -B 3 Note in the output the value of lbPolicy should say LEAST_REQUEST , which is Envoy's name for Istio's LEAST_CONN setting. Verify that the Envoy configuration was altered and that client calls now follow the \"least request\" algorithm. Scenario 2: Two clusters with routing configuration \u00b6 Scale back the httpbin deployment to a single replica: kubectl scale deploy httpbin --replicas = 1 Deploy a second httpbin service \u00b6 The following manifest is a separate deployment of httpbin , named httpbin-2 . httpbin-2.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # httpbin service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : httpbin-2 --- apiVersion : v1 kind : Service metadata : name : httpbin-2 labels : app : httpbin-2 service : httpbin-2 spec : ports : - name : http port : 8000 targetPort : 80 selector : app : httpbin-2 --- apiVersion : apps/v1 kind : Deployment metadata : name : httpbin-2 spec : replicas : 1 selector : matchLabels : app : httpbin-2 version : v1 template : metadata : labels : app : httpbin-2 version : v1 spec : serviceAccountName : httpbin-2 containers : - image : docker.io/kennethreitz/httpbin imagePullPolicy : IfNotPresent name : httpbin ports : - containerPort : 80 kubectl apply -f httpbin-2.yaml Apply the routing configuration: VirtualService \u00b6 If you recall, back in the Envoy lab, you wrote Envoy routing configuration involving path prefixes and rewrites. In Istio, the routing configuration is exposed as a Kubernetes custom resource of kind VirtualService . Study the manifest shown below: virtual-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : httpbin-vs spec : hosts : - httpbin.default.svc.cluster.local http : - match : - uri : prefix : \"/one\" rewrite : uri : \"/ip\" route : - destination : host : httpbin.default.svc.cluster.local - match : - uri : prefix : \"/two\" rewrite : uri : \"/user-agent\" route : - destination : host : httpbin-2.default.svc.cluster.local It states: when making requests to the httpbin host, route the request to either the first destination ( httpbin ) or the second ( httpbin-2 ), as a function of the path prefix in the request URL. Apply the manifest: kubectl apply -f virtual-service.yaml Verify \u00b6 Verify that requests to /one are routed to the httpbin deployment's /ip endpoint, and that requests to /two are routed to the httpbin-2 deployment's /user-agent endpoint. Tail the logs of the httpbin pod's istio-proxy container: HTTPBIN_POD = $( kubectl get pod -l app = httpbin -ojsonpath = '{.items[0].metadata.name}' ) kubectl logs --follow $HTTPBIN_POD -c istio-proxy In a separate terminal, tail the httpbin-2 pod's logs: HTTPBIN2_POD = $( kubectl get pod -l app = httpbin-2 -ojsonpath = '{.items[0].metadata.name}' ) kubectl logs --follow $HTTPBIN2_POD -c istio-proxy Separately, make repeated calls to the /one endpoint from the sleep pod: SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) kubectl exec $SLEEP_POD -it -- curl httpbin:8000/one Likewise, make repeated calls to the /two endpoint from the sleep pod: SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) kubectl exec $SLEEP_POD -it -- curl httpbin:8000/two Using an Ingress Gateway \u00b6 Rather than configure routing for internal mesh clients, it's more interesting to configure an ingress gateway. Indeed when installing Istio, an ingress gateway was provisioned alongside istiod . Verify this: kubectl get pod -n istio-system Note that the gateway has a corresponding LoadBalancer type service: kubectl get svc -n istio-system Capture the gateway's external IP address: GATEWAY_IP = $( kubectl get service istio-ingressgateway -n istio-system -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) Visit the gateway IP address in your web browser; you should get back a \"connection refused\" message. Configure the gateway \u00b6 To expose HTTP port 80, apply the following gateway manifest: gateway.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : frontend-gateway spec : selector : istio : ingressgateway servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" The wildcard value for the hosts field ensures a match if the request is made directly to the \"raw\" gateway IP address. kubectl apply -f gateway.yaml Try once more to access the gateway IP address. It should no longer return \"connection refused\". Instead you should get a 404 (not found). Bind the virtual service to the gateway \u00b6 Study the following manifest: gw-virtual-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : httpbin-vs spec : hosts : - \"*\" gateways : - frontend-gateway http : - match : - uri : prefix : \"/one\" rewrite : uri : \"/ip\" route : - destination : host : httpbin.default.svc.cluster.local - match : - uri : prefix : \"/two\" rewrite : uri : \"/user-agent\" route : - destination : host : httpbin-2.default.svc.cluster.local Note: The additional gateways field ensures that the virtual service binds to the ingress gateway. The hosts field has been relaxed to match any request coming in through the load balancer. Apply the manifest: kubectl apply -f gw-virtual-service.yaml Test the endpoints \u00b6 The raw gateway IP address will still return a 404. However, the /one and /two endpoints should now be functional, and return the ip and user-agent responses from each httpbin deployment, respectively. Inspect the Gateway's Envoy configuration \u00b6 Review the listeners configuration. istioctl proxy-config listener deploy/istio-ingressgateway.istio-system Next study the routes configuration. istioctl proxy-config route deploy/istio-ingressgateway.istio-system Zero-in on the routes configuration named http.8080 istioctl proxy-config route deploy/istio-ingressgateway.istio-system --name http.8080 -o yaml It's worthwhile taking a close look at the output. Below I have removed some of the noise to highlight the most salient parts: ... routes : - ... match : ... prefix : /one ... route : cluster : outbound|8000||httpbin.default.svc.cluster.local ... prefixRewrite : /ip ... - ... match : ... prefix : /two ... route : cluster : outbound|8000||httpbin-2.default.svc.cluster.local ... prefixRewrite : /user-agent ... Challenge Review the hand-written configuration from the previous lab. How does it compare to the above generated configuration? Beyond traffic management \u00b6 The ability to control load-balancing and routing are but one of the features of Istio. Istio supports additional and important cross-cutting concerns, including security and observability . Security \u00b6 With Istio, deployed workloads are automatically assigned a unique identity. Istio provides the PeerAuthentication CRD to control whether traffic within the mesh require mutual TLS exclusively, or whether it should be permissive. The RequestAuthentication CRD is used to turn on parsing and validation of JWT tokens. Workload and user identity are the the basis for authentication. The AuthorizationPolicy CRD provides powerful mechanism for applying authorization policies based on either workload or user identity, as well as arbitrary information from the request, such as specific request headers, JWT claims, and more. Explore observability \u00b6 In a microservices architecture, observability is necessary to help us reason about our systems, how calls traverse our microservices, to identify bottlenecks, and more. The services in an Istio mesh are automatically observable, without adding any burden on developers. Deploy the Addons \u00b6 The Istio distribution provides addons for a number of systems that together provide observability for the service mesh: Zipkin or Jaeger for distributed tracing Prometheus for metrics collection Grafana provides dashboards for monitoring, using Prometheus as the data source Kiali allows us to visualize the mesh These addons are located in the samples/addons/ folder of the distribution. Navigate to the addons directory cd ~/istio-1.13.3/samples/addons Deploy each addon: kubectl apply -f extras/zipkin.yaml kubectl apply -f prometheus.yaml kubectl apply -f grafana.yaml kubectl apply -f kiali.yaml Verify that the istio-system namespace is now running additional workloads for each of the addons. kubectl get pod -n istio-system Generate a load \u00b6 Recall the ingress gateway IP address from the previous section: GATEWAY_IP = $( kubectl get service istio-ingressgateway -n istio-system -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) In order to have something to observe, we need to generate a load on our system. for (( i = 1 ; i< = 600 ; i++ )) ; do curl $GATEWAY_IP /one ; curl $GATEWAY_IP /two ; sleep 1 ; done Explore the dashboards \u00b6 The istioctl CLI provides convenience commands for accessing the web UIs for each dashboard. Take a moment to review the help information for the istioctl dashboard command: istioctl dashboard --help Proceed to explore each of the dashboards using the above command. Your instructor will give a demonstration of each dashboard, time permitting. Summary \u00b6 In comparison to having to configure Envoy proxies manually, Istio provides a mechanism to configure Envoy proxies with much less effort. It draws on information from the environment: awareness of running workloads (service discovery) provides the inputs necessary to derive Envoy's clusters and listeners configurations automatically. Istio Custom Resource Definitions complement and complete the picture by providing mechanisms to configure routing rules, authorization policies and more. Istio goes one step further: it dynamically reconfigures the Envoy proxies any time that services are scaled, or added to and removed from the mesh. Istio and Envoy together provide a foundation for running microservices at scale. In the next lab, we turn our attention to Web Assembly, a mechanism for extending and customizing the behavior of the Envoy proxies running in the mesh.","title":"Introduction to Istio"},{"location":"istio/#introduction-to-istio","text":"","title":"Introduction to Istio"},{"location":"istio/#preface","text":"In the Envoy lab, we explored two scenarios: A single Envoy \"cluster\" with two endpoints. In this scenario, we observed that a request to the proxy resulted in the load-balancing of requests across the two endpoints. Two Envoy clusters, together with a routing configuration to route requests from the proxy to either cluster depending on the request's path prefix: Requests having the path prefix of /one were routed to the first cluster's /ip endpoint, and Requests with the path prefix of /two were routed to the second cluster's /user-agent endpoint. In this lab, you will learn how to model both scenarios in the context of Istio. Envoy is a building block of Istio. In Istio, Envoy proxies are configured indirectly, using a combination of: Implicit information drawn from the Kubernetes environment, and Istio-specific Kubernetes custom resources.","title":"Preface"},{"location":"istio/#environments","text":"See options for environments.","title":"Environments"},{"location":"istio/#install-istio","text":"Follow these instructions to install Istio in your environment.","title":"Install Istio"},{"location":"istio/#download-lab-artifacts","text":"Use the following command to download (to a subdirectory named istio-artifacts ) a copy of all yaml manifests necessary for this lab. git clone https://github.com/tetratelabs/kubecon2022-eu-immersion-day.git && \\ mv kubecon2022-eu-immersion-day/artifacts/istio ./istio-artifacts && \\ rm -rf kubecon2022-eu-immersion-day","title":"Download lab artifacts"},{"location":"istio/#where-are-the-envoys","text":"In Istio, Envoy proxy instances are present in two distinct locations: In the heart of the mesh : they are bundled as sidecar containers in the pods that run our workloads. At the edge : as standalone gateways handling ingress and egress traffic in and out of the mesh. An ingress gateway is deployed as part of the installation of Istio. It resides in the istio-system namespace. Verify this: kubectl get deploy -n istio-system To deploy Envoy as a sidecar, we will employ the convenient automatic sidecar injection , which works as follows: Label the target namespace with the special label istio-injection with the value enabled : kubectl label ns default istio-injection = enabled Verify: kubectl get ns -Listio-injection When using kubectl to apply a deployment, Istio employs a Kubernetes admission controller to augment the pod specification to bundle Envoy into a sidecar container. Verify this: observe the presence of the istio sidecar injector in your Kubernetes cluster: kubectl get mutatingwebhookconfigurations","title":"Where are the Envoys?"},{"location":"istio/#turn-on-envoy-access-logging","text":"Turn on access logging in Envoy, by applying the following Telemetry custom resource: access-logging.yaml 1 2 3 4 5 6 7 8 9 apiVersion : telemetry.istio.io/v1alpha1 kind : Telemetry metadata : name : mesh-default namespace : istio-system spec : accessLogging : - providers : - name : envoy kubectl apply -f access-logging.yaml This will simplify our ability to observe http requests in the mesh. What is Telemetry resource? The Telemetry resource is a Kubernetes custom resource that defines how the telemetry is generated for workloads within the mesh.","title":"Turn on Envoy access logging"},{"location":"istio/#scenario-1-load-balancing-across-multiple-endpoints","text":"","title":"Scenario 1: Load-balancing across multiple endpoints"},{"location":"istio/#deploy-httpbin","text":"As in the previous lab, we use httpbin as the application under test. Istio conveniently provides httpbin as one of its sample applications . For convenience, you will find a copy of the httpbin.yaml Kubernetes manifest in the istio-artifacts folder. Deploy httpbin to the default namespace: httpbin.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # httpbin service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : httpbin --- apiVersion : v1 kind : Service metadata : name : httpbin labels : app : httpbin service : httpbin spec : ports : - name : http port : 8000 targetPort : 80 selector : app : httpbin --- apiVersion : apps/v1 kind : Deployment metadata : name : httpbin spec : replicas : 1 selector : matchLabels : app : httpbin version : v1 template : metadata : labels : app : httpbin version : v1 spec : serviceAccountName : httpbin containers : - image : docker.io/kennethreitz/httpbin imagePullPolicy : IfNotPresent name : httpbin ports : - containerPort : 80 kubectl apply -f httpbin.yaml","title":"Deploy httpbin"},{"location":"istio/#scale-httpbin","text":"kubectl scale deploy httpbin --replicas = 2 Having two pods will give us the two endpoints to load-balance across.","title":"Scale httpbin"},{"location":"istio/#deploy-the-sleep-client","text":"Istio also provides a convenient sample app named sleep . Deploy the sleep client: sleep.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # Sleep service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : sleep --- apiVersion : v1 kind : Service metadata : name : sleep labels : app : sleep service : sleep spec : ports : - port : 80 name : http selector : app : sleep --- apiVersion : apps/v1 kind : Deployment metadata : name : sleep spec : replicas : 1 selector : matchLabels : app : sleep template : metadata : labels : app : sleep spec : terminationGracePeriodSeconds : 0 serviceAccountName : sleep containers : - name : sleep image : curlimages/curl command : [ \"/bin/sleep\" , \"3650d\" ] imagePullPolicy : IfNotPresent volumeMounts : - mountPath : /etc/sleep/tls name : secret-volume volumes : - name : secret-volume secret : secretName : sleep-secret optional : true --- kubectl apply -f sleep.yaml","title":"Deploy the sleep client"},{"location":"istio/#challenge","text":"Observe that all pods in the default namespace each have two containers: kubectl get pod -n default Can you discover the name of the sidecar container? Hint Describe any of the pods in the default namespace and study the Containers section.","title":"Challenge"},{"location":"istio/#observe-load-balancing-between-the-two-endpoints","text":"Requests from sleep are load-balanced across the two httpbin endpoints. Note In the commands below, we capture the names of each of the two httpbin pods and of the sleep pod independently, for clarity. Tail the logs of each Envoy sidecar on the receiving end. In one terminal, run: HTTPBIN_POD_1 = $( kubectl get pod -l app = httpbin -ojsonpath = '{.items[0].metadata.name}' ) kubectl logs --follow $HTTPBIN_POD_1 -c istio-proxy Note Note above how the name of the container istio-proxy is used to reference the sidecar. In a second terminal, run: HTTPBIN_POD_2 = $( kubectl get pod -l app = httpbin -ojsonpath = '{.items[1].metadata.name}' ) kubectl logs --follow $HTTPBIN_POD_2 -c istio-proxy Make repeated calls from the sleep container to the httbin service and observe which of the two httpbin pods receives the request. SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) kubectl exec $SLEEP_POD -it -- curl httpbin:8000/html You can stop following the logs by pressing Ctrl + C and close the first two terminal windows.","title":"Observe load-balancing between the two endpoints"},{"location":"istio/#behind-the-curtain","text":"The Istio CLI, istioctl , provides a handy subcommand proxy-config , that will help us get at the configuration of the Envoy proxy in the sleep pod: its listeners, routes, clusters, and endpoints. Capture the name of the sleep pod to a variable: SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' )","title":"Behind the curtain"},{"location":"istio/#envoys-listeners-configuration","text":"Run the following command: istioctl proxy-config listener $SLEEP_POD The output displays a high-level overview of the Envoy listener configuration. From this output we learn that Envoy has multiple listeners, listening on multiple ports. Some listeners handle inbound requests, for example there's a health endpoint on port 15021 , and a prometheus scrape endpoint on port 15090 . The listener on port 8000 (which matches the port number of the httpbin cluster IP service) is responsible for handling requests bound to the httpbin service. To see the full listener section of the Envoy configuration for port 8000 , run: istioctl proxy-config listener $SLEEP_POD --port 8000 -o yaml > listeners.yaml The output is voluminous (~ 200+ lines) and that's why we piped it into the listeners.yaml file. Note the following: trafficDirection (at the very end of the output) is set to OUTBOUND The address section specifies the address and port that the listener is configured for: address : socketAddress : address : 0.0.0.0 portValue : 8000 The configuration contains a filterChains field: filterChains : - filterChainMatch : applicationProtocols : ... The filter chain contains a filter named envoy.filters.network.http_connection_manager , and its list of httpFilters ends with the router filter: httpFilters : - name : istio.metadata_exchange - ... - name : envoy.filters.http.router typedConfig : '@type' : type.googleapis.com/envoy.extensions.filters.http.router.v3.Router All of the above facts should match with what you learned in the Introduction to Envoy .","title":"Envoy's listeners configuration"},{"location":"istio/#routes","text":"Similar to the proxy-config listener command, the high-level overview for routes is the following command: istioctl proxy-config route $SLEEP_POD Zero-in on the route configuration for port 8000 : istioctl proxy-config route $SLEEP_POD --name 8000 -o yaml The output will show the route configuration, including this section: ... routes : - decorator : operation : httpbin.default.svc.cluster.local:8000/* match : prefix : / name : default route : cluster : outbound|8000||httpbin.default.svc.cluster.local ... The above output states that calls to the httpbin service should be routed to the cluster named outbound|8000||httpbin.default.svc.cluster.local .","title":"Routes"},{"location":"istio/#clusters","text":"We can view all Envoy clusters with: istioctl proxy-config cluster $SLEEP_POD And specifically look at the configuration for the httpbin cluster with: istioctl proxy-config cluster $SLEEP_POD --fqdn httpbin.default.svc.cluster.local -o yaml","title":"Clusters"},{"location":"istio/#endpoints","text":"More importantly, we'd like to know what are the endpoints backing the httpbin cluster. istioctl proxy-config endpoint $SLEEP_POD --cluster \"outbound|8000||httpbin.default.svc.cluster.local\" Verify that the endpoint addresses from the output in fact match the pod IPs of the httpbin workloads: kubectl get pod -l app = httpbin -o wide","title":"Endpoints"},{"location":"istio/#destination-rules","text":"With Istio, you can apply the DestinationRule CRD (Custom Resource Definition) to configure traffic policy: the details of how clients call a service. Specifically, you can configure: Load balancer settings : which load balancing algorithm to use Connection pool settings : for both tcp and http connections, configure the volume of connections, retries, timeouts, etc.. Outlier detection : under what conditions to evict an unhealthy endpoints, and for how long TLS mode : whether a connection to an upstream service should use plain text, TLS, mutual TLS using certificates you specify, or mutual TLS using Istio-issued certificates. Explore applying a destination rule to alter the load balancer configuration. Did you know? What is the default load balancing algorithm currently in play for calls to httpbin ? Visit the Istio configuration reference here to find out. Apply the following destination rule for the httpbin service, which alters the load balancing algorithm to LEAST_CONN : destination-rule.yaml 1 2 3 4 5 6 7 8 9 10 --- apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : httbin spec : host : httpbin.default.svc.cluster.local trafficPolicy : loadBalancer : simple : LEAST_CONN In Envoy, the load balancer policy is associated to a given upstream service, in Envoy's terms, it's in the \"cluster\" config. Look for lbPolicy field in cluster configuration YAML output: istioctl proxy-config cluster $SLEEP_POD --fqdn httpbin.default.svc.cluster.local -o yaml | grep lbPolicy -A 3 -B 3 Note in the output the value of lbPolicy should say LEAST_REQUEST , which is Envoy's name for Istio's LEAST_CONN setting. Verify that the Envoy configuration was altered and that client calls now follow the \"least request\" algorithm.","title":"Destination Rules"},{"location":"istio/#scenario-2-two-clusters-with-routing-configuration","text":"Scale back the httpbin deployment to a single replica: kubectl scale deploy httpbin --replicas = 1","title":"Scenario 2: Two clusters with routing configuration"},{"location":"istio/#deploy-a-second-httpbin-service","text":"The following manifest is a separate deployment of httpbin , named httpbin-2 . httpbin-2.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # httpbin service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : httpbin-2 --- apiVersion : v1 kind : Service metadata : name : httpbin-2 labels : app : httpbin-2 service : httpbin-2 spec : ports : - name : http port : 8000 targetPort : 80 selector : app : httpbin-2 --- apiVersion : apps/v1 kind : Deployment metadata : name : httpbin-2 spec : replicas : 1 selector : matchLabels : app : httpbin-2 version : v1 template : metadata : labels : app : httpbin-2 version : v1 spec : serviceAccountName : httpbin-2 containers : - image : docker.io/kennethreitz/httpbin imagePullPolicy : IfNotPresent name : httpbin ports : - containerPort : 80 kubectl apply -f httpbin-2.yaml","title":"Deploy a second httpbin service"},{"location":"istio/#apply-the-routing-configuration-virtualservice","text":"If you recall, back in the Envoy lab, you wrote Envoy routing configuration involving path prefixes and rewrites. In Istio, the routing configuration is exposed as a Kubernetes custom resource of kind VirtualService . Study the manifest shown below: virtual-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : httpbin-vs spec : hosts : - httpbin.default.svc.cluster.local http : - match : - uri : prefix : \"/one\" rewrite : uri : \"/ip\" route : - destination : host : httpbin.default.svc.cluster.local - match : - uri : prefix : \"/two\" rewrite : uri : \"/user-agent\" route : - destination : host : httpbin-2.default.svc.cluster.local It states: when making requests to the httpbin host, route the request to either the first destination ( httpbin ) or the second ( httpbin-2 ), as a function of the path prefix in the request URL. Apply the manifest: kubectl apply -f virtual-service.yaml","title":"Apply the routing configuration: VirtualService"},{"location":"istio/#verify","text":"Verify that requests to /one are routed to the httpbin deployment's /ip endpoint, and that requests to /two are routed to the httpbin-2 deployment's /user-agent endpoint. Tail the logs of the httpbin pod's istio-proxy container: HTTPBIN_POD = $( kubectl get pod -l app = httpbin -ojsonpath = '{.items[0].metadata.name}' ) kubectl logs --follow $HTTPBIN_POD -c istio-proxy In a separate terminal, tail the httpbin-2 pod's logs: HTTPBIN2_POD = $( kubectl get pod -l app = httpbin-2 -ojsonpath = '{.items[0].metadata.name}' ) kubectl logs --follow $HTTPBIN2_POD -c istio-proxy Separately, make repeated calls to the /one endpoint from the sleep pod: SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) kubectl exec $SLEEP_POD -it -- curl httpbin:8000/one Likewise, make repeated calls to the /two endpoint from the sleep pod: SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) kubectl exec $SLEEP_POD -it -- curl httpbin:8000/two","title":"Verify"},{"location":"istio/#using-an-ingress-gateway","text":"Rather than configure routing for internal mesh clients, it's more interesting to configure an ingress gateway. Indeed when installing Istio, an ingress gateway was provisioned alongside istiod . Verify this: kubectl get pod -n istio-system Note that the gateway has a corresponding LoadBalancer type service: kubectl get svc -n istio-system Capture the gateway's external IP address: GATEWAY_IP = $( kubectl get service istio-ingressgateway -n istio-system -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) Visit the gateway IP address in your web browser; you should get back a \"connection refused\" message.","title":"Using an Ingress Gateway"},{"location":"istio/#configure-the-gateway","text":"To expose HTTP port 80, apply the following gateway manifest: gateway.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : frontend-gateway spec : selector : istio : ingressgateway servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" The wildcard value for the hosts field ensures a match if the request is made directly to the \"raw\" gateway IP address. kubectl apply -f gateway.yaml Try once more to access the gateway IP address. It should no longer return \"connection refused\". Instead you should get a 404 (not found).","title":"Configure the gateway"},{"location":"istio/#bind-the-virtual-service-to-the-gateway","text":"Study the following manifest: gw-virtual-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : httpbin-vs spec : hosts : - \"*\" gateways : - frontend-gateway http : - match : - uri : prefix : \"/one\" rewrite : uri : \"/ip\" route : - destination : host : httpbin.default.svc.cluster.local - match : - uri : prefix : \"/two\" rewrite : uri : \"/user-agent\" route : - destination : host : httpbin-2.default.svc.cluster.local Note: The additional gateways field ensures that the virtual service binds to the ingress gateway. The hosts field has been relaxed to match any request coming in through the load balancer. Apply the manifest: kubectl apply -f gw-virtual-service.yaml","title":"Bind the virtual service to the gateway"},{"location":"istio/#test-the-endpoints","text":"The raw gateway IP address will still return a 404. However, the /one and /two endpoints should now be functional, and return the ip and user-agent responses from each httpbin deployment, respectively.","title":"Test the endpoints"},{"location":"istio/#inspect-the-gateways-envoy-configuration","text":"Review the listeners configuration. istioctl proxy-config listener deploy/istio-ingressgateway.istio-system Next study the routes configuration. istioctl proxy-config route deploy/istio-ingressgateway.istio-system Zero-in on the routes configuration named http.8080 istioctl proxy-config route deploy/istio-ingressgateway.istio-system --name http.8080 -o yaml It's worthwhile taking a close look at the output. Below I have removed some of the noise to highlight the most salient parts: ... routes : - ... match : ... prefix : /one ... route : cluster : outbound|8000||httpbin.default.svc.cluster.local ... prefixRewrite : /ip ... - ... match : ... prefix : /two ... route : cluster : outbound|8000||httpbin-2.default.svc.cluster.local ... prefixRewrite : /user-agent ... Challenge Review the hand-written configuration from the previous lab. How does it compare to the above generated configuration?","title":"Inspect the Gateway's Envoy configuration"},{"location":"istio/#beyond-traffic-management","text":"The ability to control load-balancing and routing are but one of the features of Istio. Istio supports additional and important cross-cutting concerns, including security and observability .","title":"Beyond traffic management"},{"location":"istio/#security","text":"With Istio, deployed workloads are automatically assigned a unique identity. Istio provides the PeerAuthentication CRD to control whether traffic within the mesh require mutual TLS exclusively, or whether it should be permissive. The RequestAuthentication CRD is used to turn on parsing and validation of JWT tokens. Workload and user identity are the the basis for authentication. The AuthorizationPolicy CRD provides powerful mechanism for applying authorization policies based on either workload or user identity, as well as arbitrary information from the request, such as specific request headers, JWT claims, and more.","title":"Security"},{"location":"istio/#explore-observability","text":"In a microservices architecture, observability is necessary to help us reason about our systems, how calls traverse our microservices, to identify bottlenecks, and more. The services in an Istio mesh are automatically observable, without adding any burden on developers.","title":"Explore observability"},{"location":"istio/#deploy-the-addons","text":"The Istio distribution provides addons for a number of systems that together provide observability for the service mesh: Zipkin or Jaeger for distributed tracing Prometheus for metrics collection Grafana provides dashboards for monitoring, using Prometheus as the data source Kiali allows us to visualize the mesh These addons are located in the samples/addons/ folder of the distribution. Navigate to the addons directory cd ~/istio-1.13.3/samples/addons Deploy each addon: kubectl apply -f extras/zipkin.yaml kubectl apply -f prometheus.yaml kubectl apply -f grafana.yaml kubectl apply -f kiali.yaml Verify that the istio-system namespace is now running additional workloads for each of the addons. kubectl get pod -n istio-system","title":"Deploy the Addons"},{"location":"istio/#generate-a-load","text":"Recall the ingress gateway IP address from the previous section: GATEWAY_IP = $( kubectl get service istio-ingressgateway -n istio-system -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) In order to have something to observe, we need to generate a load on our system. for (( i = 1 ; i< = 600 ; i++ )) ; do curl $GATEWAY_IP /one ; curl $GATEWAY_IP /two ; sleep 1 ; done","title":"Generate a load"},{"location":"istio/#explore-the-dashboards","text":"The istioctl CLI provides convenience commands for accessing the web UIs for each dashboard. Take a moment to review the help information for the istioctl dashboard command: istioctl dashboard --help Proceed to explore each of the dashboards using the above command. Your instructor will give a demonstration of each dashboard, time permitting.","title":"Explore the dashboards"},{"location":"istio/#summary","text":"In comparison to having to configure Envoy proxies manually, Istio provides a mechanism to configure Envoy proxies with much less effort. It draws on information from the environment: awareness of running workloads (service discovery) provides the inputs necessary to derive Envoy's clusters and listeners configurations automatically. Istio Custom Resource Definitions complement and complete the picture by providing mechanisms to configure routing rules, authorization policies and more. Istio goes one step further: it dynamically reconfigures the Envoy proxies any time that services are scaled, or added to and removed from the mesh. Istio and Envoy together provide a foundation for running microservices at scale. In the next lab, we turn our attention to Web Assembly, a mechanism for extending and customizing the behavior of the Envoy proxies running in the mesh.","title":"Summary"},{"location":"summary/","text":"You made it! \u00b6 Congratulations, you've made it to the end of the workshop! Additional resources \u00b6 Tetrate Community Slack Tetrate Tech Talks Tetrate Academy courses \u00b6 Istio fundamentals course Envoy fundamentals course Certified Istio Administrator by Tetrate","title":"Summary"},{"location":"summary/#you-made-it","text":"Congratulations, you've made it to the end of the workshop!","title":"You made it!"},{"location":"summary/#additional-resources","text":"Tetrate Community Slack Tetrate Tech Talks","title":"Additional resources"},{"location":"summary/#tetrate-academy-courses","text":"Istio fundamentals course Envoy fundamentals course Certified Istio Administrator by Tetrate","title":"Tetrate Academy courses"},{"location":"wasm/","text":"Extending Envoy and Istio with Wasm \u00b6 Envoy Wasm filter \u00b6 Let's look more closely at the Envoy configuration from the previous section: HTTPBIN_POD = $( kubectl get pod -l app = httpbin -ojsonpath = '{.items[0].metadata.name}' ) We'll use the /config_dump endpoint on the Envoy proxy container in the pod to get a full Envoy configuration dump: kubectl exec -it $HTTPBIN_POD -c istio-proxy -- curl localhost:15000/config_dump > envoy.json Because the configuration is enormous, let's search for the type.googleapis.com/envoy.extensions.filters.network.wasm.v3.Wasm . Here's a snippet: wasm-filter.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ... \"default_filter_chain\" : { \"filters\" : [ { \"name\" : \"istio.stats\" , \"typed_config\" : { \"@type\" : \"type.googleapis.com/udpa.type.v1.TypedStruct\" , \"type_url\" : \"type.googleapis.com/envoy.extensions.filters.network.wasm.v3.Wasm\" , \"value\" : { \"config\" : { \"root_id\" : \"stats_outbound\" , \"vm_config\" : { \"vm_id\" : \"tcp_stats_outbound\" , \"runtime\" : \"envoy.wasm.runtime.null\" , \"code\" : { \"local\" : { \"inline_string\" : \"envoy.wasm.stats\" } } ... Note There will be more than one instance of the type.googleapis.com/envoy.extensions.filters.network.wasm.v3.Wasm filter in the configuration. The envoy.wasm.stats extension gets executed on multiple paths for multiple listeners. The istio.stats extension is a Wasm extension built into Envoy. How do we know that? Well, the built-in extensions use the envoy.wasm.runtime.null runtime. If we wanted to run our Wasm extension, we could bundle it with Envoy. However, there are easier ways to do this. We can tell Envoy to load a Wasm extension from a specific .wasm file we provide in the configuration. We don't have to rebuild Envoy and maintain our Envoy binary. Using the Wasm filter \u00b6 To configure a Wasm extension, we use a HTTP filter called envoy.extensions.filters.network.wasm.v3.Wasm . Since this is a HTTP filter, we know we have to configure it inside the http_filters section right before the router filter ( envoy.filters.http.router ). Let's use the Envoy configuration we're already familiar with and see if we can figure out how to configure Envoy to load a Wasm extension. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ... filter_chains : - filters : - name : envoy.filters.network.http_connection_manager typed_config : \"@type\" : type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix : hello_world_service http_filters : - name : envoy.filters.http.wasm typed_config : \"@type\" : type.googleapis.com/udpa.type.v1.TypedStruct type_url : type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm value : config : vm_config : vm_id : \"my_vm\" runtime : \"envoy.wasm.runtime.v8\" # (1) code : local : filename : \"main.wasm\" # (2) - name : envoy.filters.http.router route_config : ... To tell Envoy our extension is not built-in, we use the envoy.wasm.runtime.v8 runtime. We provide the main.wasm file that contains our extension. Note that we could replace local with remote and point to an URL instead. Make sure you run the two Docker containers from the first section: docker run -d -p 8100 :80 kennethreitz/httpbin docker run -d -p 8200 :80 kennethreitz/httpbin Because we'll be building a Wasm extension, let's create a separate folder for it so that we can store all files in the same place: mkdir wasm-extension && cd wasm-extension We can now run func-e with the following configuration: envoy-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 static_resources : listeners : - name : listener_0 address : socket_address : address : 0.0.0.0 port_value : 10000 filter_chains : - filters : - name : envoy.filters.network.http_connection_manager typed_config : \"@type\" : type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix : hello_world_service http_filters : - name : envoy.filters.http.wasm typed_config : \"@type\" : type.googleapis.com/udpa.type.v1.TypedStruct type_url : type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm value : config : vm_config : vm_id : \"my_vm\" runtime : \"envoy.wasm.runtime.v8\" code : local : filename : \"main.wasm\" - name : envoy.filters.http.router route_config : name : my_first_route virtual_hosts : - name : my_vhost domains : [ \"*\" ] routes : - match : path : \"/one\" route : prefix_rewrite : \"/ip\" cluster : hello_world_cluster - match : path : \"/two\" route : prefix_rewrite : \"/user-agent\" cluster : second_cluster clusters : - name : hello_world_cluster connect_timeout : 5s load_assignment : cluster_name : hello_world_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8100 - name : second_cluster connect_timeout : 5s load_assignment : cluster_name : second_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8200 func-e run -c envoy-config.yaml [2022-04-28 19:03:46.607][2672][critical][main] [source/server/server.cc:114] error initializing configuration 'envoy-config.yaml': Invalid path: main.wasm [2022-04-28 19:03:46.607][2672][info][main] [source/server/server.cc:891] exiting Invalid path: main.wasm It should fail because there's no main.wasm file. Let's build one! Building a Wasm Extension \u00b6 We'll build a simple Wasm extension that adds a custom response HTTP header to all requests. From the wasm-extension folder, let's initialize the Go module: go mod init wasm-extension Next, let's create the main.go file where the code for our Wasm extension will live: main.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 package main import ( \"github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\" \"github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\" ) func main () { proxywasm . SetVMContext ( & vmContext {}) } type vmContext struct { // Embed the default VM context here, // so that we don't need to reimplement all the methods. types . DefaultVMContext } // Override types.DefaultVMContext. func ( * vmContext ) NewPluginContext ( contextID uint32 ) types . PluginContext { return & pluginContext {} } type pluginContext struct { // Embed the default plugin context here, // so that we don't need to reimplement all the methods. types . DefaultPluginContext } // Override types.DefaultPluginContext. func ( * pluginContext ) NewHttpContext ( contextID uint32 ) types . HttpContext { proxywasm . LogInfo ( \"NewHttpContext\" ) return & httpContext { contextID : contextID } } func ( ctx * httpContext ) OnHttpResponseHeaders ( numHeaders int , endOfStream bool ) types . Action { proxywasm . LogInfo ( \"OnHttpResponseHeaders\" ) key := \"x-custom-header\" value := \"custom-value\" if err := proxywasm . AddHttpResponseHeader ( key , value ); err != nil { proxywasm . LogCriticalf ( \"failed to add header: %v\" , err ) return types . ActionPause } proxywasm . LogInfof ( \"header set: %s=%s\" , key , value ) return types . ActionContinue } type httpContext struct { // Embed the default http context here, // so that we don't need to reimplement all the methods. types . DefaultHttpContext contextID uint32 } Save the above to main.go . In the main.go file we defined a couple of functions that will be called by Envoy when the extension is loaded or when the requests are being processed. The part where we add the custom response header is in the OnHttpResponseHeaders function, as shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 func ( ctx * httpContext ) OnHttpResponseHeaders ( numHeaders int , endOfStream bool ) types . Action { proxywasm . LogInfo ( \"OnHttpResponseHeaders\" ) // (1) key := \"x-custom-header\" value := \"custom-value\" if err := proxywasm . AddHttpResponseHeader ( key , value ); err != nil { // (2) proxywasm . LogCriticalf ( \"failed to add header: %v\" , err ) return types . ActionPause // (3) } proxywasm . LogInfof ( \"header set: %s=%s\" , key , value ) return types . ActionContinue // (4) } ProxyWasm library has built-in functions for logging. We can AddHttpResponseHeader to add a custom response header. In case of an error, we return types.ActionPause to tell Envoy to stop executing subsequent filters. If there are no errors, we continue with the execution. Proxy Wasm Go SDK API The SDK API is in the proxywasm package included in the source code. The SDK provides a set of functions we can use to interact with the Envoy proxy and/or the requests and responses. It contains functions for adding and manipulating HTTP headers, body, logging functions, and other APIs for using shared queues, shared data, and more. To build the extension, we'll use the TinyGo compiler - follow these instructions to install TinyGo. With TinyGo installed, we can download the dependencies and build the extension: go mod tidy tinygo build -o main.wasm -scheduler = none -target = wasi main.go go: finding module for package github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types go: finding module for package github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm go: found github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm in github.com/tetratelabs/proxy-wasm-go-sdk v0.17.0 go: found github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types in github.com/tetratelabs/proxy-wasm-go-sdk v0.17.0 The build command should run successfully and generate a main.wasm file. We already have the Envoy config, so let's re-run func-e : func-e run -c envoy-config.yaml & This time we won't get any errors because the main.wasm file we referenced in the configuration exists. Let's try sending a couple of requests to localhost:10000/one to see the custom header we added to the response and the log entries. curl -v http://localhost:10000/one 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 * Trying 127.0.0.1:10000... * Connected to localhost (127.0.0.1) port 10000 (#0) > GET /one HTTP/1.1 > Host: localhost:10000 > User-Agent: curl/7.74.0 > Accept: */* > [2022-04-28 19:19:39.191][4295][info][wasm] [source/extensions/common/wasm/context.cc:1167] wasm log my_vm: NewHttpContext [2022-04-28 19:19:39.194][4295][info][wasm] [source/extensions/common/wasm/context.cc:1167] wasm log my_vm: OnHttpResponseHeaders [2022-04-28 19:19:39.194][4295][info][wasm] [source/extensions/common/wasm/context.cc:1167] wasm log my_vm: header set: x-custom-header=custom-value * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < server: envoy < date: Thu, 28 Apr 2022 19:19:39 GMT < content-type: application/json < content-length: 29 < access-control-allow-origin: * < access-control-allow-credentials: true < x-envoy-upstream-service-time: 1 < x-custom-header: custom-value < { \"origin\": \"172.18.0.1\" } * Connection #0 to host localhost left intact Running the Wasm extension like this is helpful. However, we want to run it next to the Envoy proxies in the Istio service mesh. Istio WasmPlugin resource \u00b6 The WasmPlugin allows us to select the workloads we want to apply the Wasm module to and point to the Wasm module. The WasmPlugin resource includes a feature that enables the Istio proxy (or istio-agent) to download the Wasm file from an OCI-compliant registry. That means we can treat the Wasm files like we treat Docker images. We can push them to a registry, version them using tags, and reference them from the WasmPlugin resource. There was no need to push or publish the main.wasm file anywhere in the previous labs, as it was accessible by the Envoy proxy because everything was running locally. However, now that we want to run the Wasm module in Envoy proxies that are part of the Istio service mesh, we need to make the main.wasm file available so all those proxies can load and run it. Building the Wasm image \u00b6 Since we'll be building and pushing the Wasm file, we'll need a very minimal Dockerfile in the project: FROM scratch COPY main.wasm ./plugin.wasm This Docker file copies the main.wasm file to the container as plugin.wasm . Save the above contents to Dockerfile . Next, we can build and push the Docker image: export REPOSITORY =[ REPOSITORY ] docker build -t ${ REPOSITORY } /wasm:v1 . docker push ${ REPOSITORY } /wasm:v1 Setting up your registry You can use any OCI-compliant registry to host your Wasm files. For example, you can use Docker Hub , or if you're using GCP, you can set up the Docker registry here , by clicking the Create Repository button, selecting the Docker format and clicking Create . Then, follow the setup instructions to complete setting up the GCP registry, and don't forget to configure access control , so you can push to it and Istio can pull from it. You can also use the pre-built images that's available here: europe-west8-docker.pkg.dev/peterjs-project/kubecon2022/wasm:v1 . Creating WasmPlugin resource \u00b6 We can now create the WasmPlugin resource that tells Envoy where to download the extension and which workloads to apply it to (we'll use httpbin workload we deployed in the previous lab). WasmPlugin resource plugin.yaml 1 2 3 4 5 6 7 8 9 10 apiVersion : extensions . istio . io / v1alpha1 kind : WasmPlugin metadata : name : wasm - example namespace : default spec : selector : matchLabels : app : httpbin url : oci : //[REPOSITORY]/wasm:v1 You should update the REPOSITORY value in the url field before saving the above YAML to plugin.yaml and deploying it using kubectl apply -f plugin.yaml . Let's try out the deployed Wasm module! Capture the gateway's external IP address: GATEWAY_IP = $( kubectl get service istio-ingressgateway -n istio-system -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) Because we applied the WasmPlugin to the first httpbin deployment (see the selector labels in the WasmPlugin resource), we can send the request to $GATEWAY_IP/one : curl -v $GATEWAY_IP /one 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 > GET /one HTTP/1.1 > Host: 34.82.240.26 > User-Agent: curl/7.74.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < server: istio-envoy < date: Thu, 28 Apr 2022 19:33:48 GMT < content-type: application/json < content-length: 32 < access-control-allow-origin: * < access-control-allow-credentials: true < x-envoy-upstream-service-time: 34 < x-custom-header: custom-value < { \"origin\": \"10.138.15.210\" } Summary \u00b6 In this lab, you learned how to create and configure a Wasm extension using Go and the proxy-wasm-go-sdk. You've learned how to run a single Envoy proxy that loads a Wasm extension and use the WasmPlugin resource to deploy the Wasm extension to Envoy proxies inside the Istio service mesh.","title":"Extending Envoy and Istio with Wasm"},{"location":"wasm/#extending-envoy-and-istio-with-wasm","text":"","title":"Extending Envoy and Istio with Wasm"},{"location":"wasm/#envoy-wasm-filter","text":"Let's look more closely at the Envoy configuration from the previous section: HTTPBIN_POD = $( kubectl get pod -l app = httpbin -ojsonpath = '{.items[0].metadata.name}' ) We'll use the /config_dump endpoint on the Envoy proxy container in the pod to get a full Envoy configuration dump: kubectl exec -it $HTTPBIN_POD -c istio-proxy -- curl localhost:15000/config_dump > envoy.json Because the configuration is enormous, let's search for the type.googleapis.com/envoy.extensions.filters.network.wasm.v3.Wasm . Here's a snippet: wasm-filter.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ... \"default_filter_chain\" : { \"filters\" : [ { \"name\" : \"istio.stats\" , \"typed_config\" : { \"@type\" : \"type.googleapis.com/udpa.type.v1.TypedStruct\" , \"type_url\" : \"type.googleapis.com/envoy.extensions.filters.network.wasm.v3.Wasm\" , \"value\" : { \"config\" : { \"root_id\" : \"stats_outbound\" , \"vm_config\" : { \"vm_id\" : \"tcp_stats_outbound\" , \"runtime\" : \"envoy.wasm.runtime.null\" , \"code\" : { \"local\" : { \"inline_string\" : \"envoy.wasm.stats\" } } ... Note There will be more than one instance of the type.googleapis.com/envoy.extensions.filters.network.wasm.v3.Wasm filter in the configuration. The envoy.wasm.stats extension gets executed on multiple paths for multiple listeners. The istio.stats extension is a Wasm extension built into Envoy. How do we know that? Well, the built-in extensions use the envoy.wasm.runtime.null runtime. If we wanted to run our Wasm extension, we could bundle it with Envoy. However, there are easier ways to do this. We can tell Envoy to load a Wasm extension from a specific .wasm file we provide in the configuration. We don't have to rebuild Envoy and maintain our Envoy binary.","title":"Envoy Wasm filter"},{"location":"wasm/#using-the-wasm-filter","text":"To configure a Wasm extension, we use a HTTP filter called envoy.extensions.filters.network.wasm.v3.Wasm . Since this is a HTTP filter, we know we have to configure it inside the http_filters section right before the router filter ( envoy.filters.http.router ). Let's use the Envoy configuration we're already familiar with and see if we can figure out how to configure Envoy to load a Wasm extension. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ... filter_chains : - filters : - name : envoy.filters.network.http_connection_manager typed_config : \"@type\" : type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix : hello_world_service http_filters : - name : envoy.filters.http.wasm typed_config : \"@type\" : type.googleapis.com/udpa.type.v1.TypedStruct type_url : type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm value : config : vm_config : vm_id : \"my_vm\" runtime : \"envoy.wasm.runtime.v8\" # (1) code : local : filename : \"main.wasm\" # (2) - name : envoy.filters.http.router route_config : ... To tell Envoy our extension is not built-in, we use the envoy.wasm.runtime.v8 runtime. We provide the main.wasm file that contains our extension. Note that we could replace local with remote and point to an URL instead. Make sure you run the two Docker containers from the first section: docker run -d -p 8100 :80 kennethreitz/httpbin docker run -d -p 8200 :80 kennethreitz/httpbin Because we'll be building a Wasm extension, let's create a separate folder for it so that we can store all files in the same place: mkdir wasm-extension && cd wasm-extension We can now run func-e with the following configuration: envoy-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 static_resources : listeners : - name : listener_0 address : socket_address : address : 0.0.0.0 port_value : 10000 filter_chains : - filters : - name : envoy.filters.network.http_connection_manager typed_config : \"@type\" : type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix : hello_world_service http_filters : - name : envoy.filters.http.wasm typed_config : \"@type\" : type.googleapis.com/udpa.type.v1.TypedStruct type_url : type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm value : config : vm_config : vm_id : \"my_vm\" runtime : \"envoy.wasm.runtime.v8\" code : local : filename : \"main.wasm\" - name : envoy.filters.http.router route_config : name : my_first_route virtual_hosts : - name : my_vhost domains : [ \"*\" ] routes : - match : path : \"/one\" route : prefix_rewrite : \"/ip\" cluster : hello_world_cluster - match : path : \"/two\" route : prefix_rewrite : \"/user-agent\" cluster : second_cluster clusters : - name : hello_world_cluster connect_timeout : 5s load_assignment : cluster_name : hello_world_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8100 - name : second_cluster connect_timeout : 5s load_assignment : cluster_name : second_cluster endpoints : - lb_endpoints : - endpoint : address : socket_address : address : 127.0.0.1 port_value : 8200 func-e run -c envoy-config.yaml [2022-04-28 19:03:46.607][2672][critical][main] [source/server/server.cc:114] error initializing configuration 'envoy-config.yaml': Invalid path: main.wasm [2022-04-28 19:03:46.607][2672][info][main] [source/server/server.cc:891] exiting Invalid path: main.wasm It should fail because there's no main.wasm file. Let's build one!","title":"Using the Wasm filter"},{"location":"wasm/#building-a-wasm-extension","text":"We'll build a simple Wasm extension that adds a custom response HTTP header to all requests. From the wasm-extension folder, let's initialize the Go module: go mod init wasm-extension Next, let's create the main.go file where the code for our Wasm extension will live: main.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 package main import ( \"github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\" \"github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\" ) func main () { proxywasm . SetVMContext ( & vmContext {}) } type vmContext struct { // Embed the default VM context here, // so that we don't need to reimplement all the methods. types . DefaultVMContext } // Override types.DefaultVMContext. func ( * vmContext ) NewPluginContext ( contextID uint32 ) types . PluginContext { return & pluginContext {} } type pluginContext struct { // Embed the default plugin context here, // so that we don't need to reimplement all the methods. types . DefaultPluginContext } // Override types.DefaultPluginContext. func ( * pluginContext ) NewHttpContext ( contextID uint32 ) types . HttpContext { proxywasm . LogInfo ( \"NewHttpContext\" ) return & httpContext { contextID : contextID } } func ( ctx * httpContext ) OnHttpResponseHeaders ( numHeaders int , endOfStream bool ) types . Action { proxywasm . LogInfo ( \"OnHttpResponseHeaders\" ) key := \"x-custom-header\" value := \"custom-value\" if err := proxywasm . AddHttpResponseHeader ( key , value ); err != nil { proxywasm . LogCriticalf ( \"failed to add header: %v\" , err ) return types . ActionPause } proxywasm . LogInfof ( \"header set: %s=%s\" , key , value ) return types . ActionContinue } type httpContext struct { // Embed the default http context here, // so that we don't need to reimplement all the methods. types . DefaultHttpContext contextID uint32 } Save the above to main.go . In the main.go file we defined a couple of functions that will be called by Envoy when the extension is loaded or when the requests are being processed. The part where we add the custom response header is in the OnHttpResponseHeaders function, as shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 func ( ctx * httpContext ) OnHttpResponseHeaders ( numHeaders int , endOfStream bool ) types . Action { proxywasm . LogInfo ( \"OnHttpResponseHeaders\" ) // (1) key := \"x-custom-header\" value := \"custom-value\" if err := proxywasm . AddHttpResponseHeader ( key , value ); err != nil { // (2) proxywasm . LogCriticalf ( \"failed to add header: %v\" , err ) return types . ActionPause // (3) } proxywasm . LogInfof ( \"header set: %s=%s\" , key , value ) return types . ActionContinue // (4) } ProxyWasm library has built-in functions for logging. We can AddHttpResponseHeader to add a custom response header. In case of an error, we return types.ActionPause to tell Envoy to stop executing subsequent filters. If there are no errors, we continue with the execution. Proxy Wasm Go SDK API The SDK API is in the proxywasm package included in the source code. The SDK provides a set of functions we can use to interact with the Envoy proxy and/or the requests and responses. It contains functions for adding and manipulating HTTP headers, body, logging functions, and other APIs for using shared queues, shared data, and more. To build the extension, we'll use the TinyGo compiler - follow these instructions to install TinyGo. With TinyGo installed, we can download the dependencies and build the extension: go mod tidy tinygo build -o main.wasm -scheduler = none -target = wasi main.go go: finding module for package github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types go: finding module for package github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm go: found github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm in github.com/tetratelabs/proxy-wasm-go-sdk v0.17.0 go: found github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types in github.com/tetratelabs/proxy-wasm-go-sdk v0.17.0 The build command should run successfully and generate a main.wasm file. We already have the Envoy config, so let's re-run func-e : func-e run -c envoy-config.yaml & This time we won't get any errors because the main.wasm file we referenced in the configuration exists. Let's try sending a couple of requests to localhost:10000/one to see the custom header we added to the response and the log entries. curl -v http://localhost:10000/one 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 * Trying 127.0.0.1:10000... * Connected to localhost (127.0.0.1) port 10000 (#0) > GET /one HTTP/1.1 > Host: localhost:10000 > User-Agent: curl/7.74.0 > Accept: */* > [2022-04-28 19:19:39.191][4295][info][wasm] [source/extensions/common/wasm/context.cc:1167] wasm log my_vm: NewHttpContext [2022-04-28 19:19:39.194][4295][info][wasm] [source/extensions/common/wasm/context.cc:1167] wasm log my_vm: OnHttpResponseHeaders [2022-04-28 19:19:39.194][4295][info][wasm] [source/extensions/common/wasm/context.cc:1167] wasm log my_vm: header set: x-custom-header=custom-value * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < server: envoy < date: Thu, 28 Apr 2022 19:19:39 GMT < content-type: application/json < content-length: 29 < access-control-allow-origin: * < access-control-allow-credentials: true < x-envoy-upstream-service-time: 1 < x-custom-header: custom-value < { \"origin\": \"172.18.0.1\" } * Connection #0 to host localhost left intact Running the Wasm extension like this is helpful. However, we want to run it next to the Envoy proxies in the Istio service mesh.","title":"Building a Wasm Extension"},{"location":"wasm/#istio-wasmplugin-resource","text":"The WasmPlugin allows us to select the workloads we want to apply the Wasm module to and point to the Wasm module. The WasmPlugin resource includes a feature that enables the Istio proxy (or istio-agent) to download the Wasm file from an OCI-compliant registry. That means we can treat the Wasm files like we treat Docker images. We can push them to a registry, version them using tags, and reference them from the WasmPlugin resource. There was no need to push or publish the main.wasm file anywhere in the previous labs, as it was accessible by the Envoy proxy because everything was running locally. However, now that we want to run the Wasm module in Envoy proxies that are part of the Istio service mesh, we need to make the main.wasm file available so all those proxies can load and run it.","title":"Istio WasmPlugin resource"},{"location":"wasm/#building-the-wasm-image","text":"Since we'll be building and pushing the Wasm file, we'll need a very minimal Dockerfile in the project: FROM scratch COPY main.wasm ./plugin.wasm This Docker file copies the main.wasm file to the container as plugin.wasm . Save the above contents to Dockerfile . Next, we can build and push the Docker image: export REPOSITORY =[ REPOSITORY ] docker build -t ${ REPOSITORY } /wasm:v1 . docker push ${ REPOSITORY } /wasm:v1 Setting up your registry You can use any OCI-compliant registry to host your Wasm files. For example, you can use Docker Hub , or if you're using GCP, you can set up the Docker registry here , by clicking the Create Repository button, selecting the Docker format and clicking Create . Then, follow the setup instructions to complete setting up the GCP registry, and don't forget to configure access control , so you can push to it and Istio can pull from it. You can also use the pre-built images that's available here: europe-west8-docker.pkg.dev/peterjs-project/kubecon2022/wasm:v1 .","title":"Building the Wasm image"},{"location":"wasm/#creating-wasmplugin-resource","text":"We can now create the WasmPlugin resource that tells Envoy where to download the extension and which workloads to apply it to (we'll use httpbin workload we deployed in the previous lab). WasmPlugin resource plugin.yaml 1 2 3 4 5 6 7 8 9 10 apiVersion : extensions . istio . io / v1alpha1 kind : WasmPlugin metadata : name : wasm - example namespace : default spec : selector : matchLabels : app : httpbin url : oci : //[REPOSITORY]/wasm:v1 You should update the REPOSITORY value in the url field before saving the above YAML to plugin.yaml and deploying it using kubectl apply -f plugin.yaml . Let's try out the deployed Wasm module! Capture the gateway's external IP address: GATEWAY_IP = $( kubectl get service istio-ingressgateway -n istio-system -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) Because we applied the WasmPlugin to the first httpbin deployment (see the selector labels in the WasmPlugin resource), we can send the request to $GATEWAY_IP/one : curl -v $GATEWAY_IP /one 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 > GET /one HTTP/1.1 > Host: 34.82.240.26 > User-Agent: curl/7.74.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < server: istio-envoy < date: Thu, 28 Apr 2022 19:33:48 GMT < content-type: application/json < content-length: 32 < access-control-allow-origin: * < access-control-allow-credentials: true < x-envoy-upstream-service-time: 34 < x-custom-header: custom-value < { \"origin\": \"10.138.15.210\" }","title":"Creating WasmPlugin resource"},{"location":"wasm/#summary","text":"In this lab, you learned how to create and configure a Wasm extension using Go and the proxy-wasm-go-sdk. You've learned how to run a single Envoy proxy that loads a Wasm extension and use the WasmPlugin resource to deploy the Wasm extension to Envoy proxies inside the Istio service mesh.","title":"Summary"}]}